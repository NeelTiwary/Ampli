{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8db2e4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T14:57:35.975012Z",
     "iopub.status.busy": "2025-05-26T14:57:35.974707Z",
     "iopub.status.idle": "2025-05-26T14:57:35.980824Z",
     "shell.execute_reply": "2025-05-26T14:57:35.979840Z"
    },
    "papermill": {
     "duration": 0.015203,
     "end_time": "2025-05-26T14:57:35.982341",
     "exception": false,
     "start_time": "2025-05-26T14:57:35.967138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.metrics import f1_score, classification_report\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Load data\n",
    "# amplitude_df = pd.read_csv(\"/kaggle/input/amplifreq/all_signals.csv\", header=None)\n",
    "# frequency_df = pd.read_csv(\"/kaggle/input/amplifreq/all_frequencies.csv\", header=None)\n",
    "\n",
    "# # Step 1: Prepare amplitude input (X)\n",
    "# X = amplitude_df.iloc[:, 1:].values.astype(np.float32)  # Keep time column in sync but skip it for training\n",
    "# X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)  # Normalize each sample\n",
    "\n",
    "# # Step 2: Prepare frequency labels (y)\n",
    "# y_raw = frequency_df.iloc[:, 1:].values  # Skip first column only for labeling\n",
    "\n",
    "# # Extract actual frequency values while keeping time 0s intact\n",
    "# label_sets = []\n",
    "# for row in y_raw:\n",
    "#     freqs = [val for val in row if val not in [0, 50, 150, 250, 350, 450, 550, 650, 750, 881, 950]]  # Keep actual frequency values only\n",
    "#     label_sets.append(list(set(freqs)))  # Unique labels per row\n",
    "\n",
    "# # Multi-label binarization\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# y = mlb.fit_transform(label_sets)\n",
    "\n",
    "# # Step 3: Tensor conversion\n",
    "# X_tensor = torch.tensor(X)\n",
    "# y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# # Step 4: Dataset and Dataloader\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# # Step 5: Define model\n",
    "# class FrequencyClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, num_labels):\n",
    "#         super(FrequencyClassifier, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_size, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, num_labels),\n",
    "#             nn.Sigmoid()  # Multi-label classification\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# # Initialize model\n",
    "# model = FrequencyClassifier(X.shape[1], y.shape[1])\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Step 6: Training loop\n",
    "# for epoch in range(350):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}/20, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# # Step 7: Evaluation\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         outputs = model(inputs)\n",
    "#         preds = (outputs > 0.5).int()\n",
    "#         all_preds.extend(preds.numpy())\n",
    "#         all_labels.extend(labels.numpy())\n",
    "\n",
    "# f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "# print(f\"\\nMicro F1 Score: {f1:.4f}\")\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(label) for label in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c60597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T14:57:35.992510Z",
     "iopub.status.busy": "2025-05-26T14:57:35.992176Z",
     "iopub.status.idle": "2025-05-26T14:57:35.995853Z",
     "shell.execute_reply": "2025-05-26T14:57:35.995016Z"
    },
    "papermill": {
     "duration": 0.010328,
     "end_time": "2025-05-26T14:57:35.997397",
     "exception": false,
     "start_time": "2025-05-26T14:57:35.987069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077f5e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T14:57:36.008008Z",
     "iopub.status.busy": "2025-05-26T14:57:36.007706Z",
     "iopub.status.idle": "2025-05-26T14:59:15.204169Z",
     "shell.execute_reply": "2025-05-26T14:59:15.203219Z"
    },
    "papermill": {
     "duration": 99.204639,
     "end_time": "2025-05-26T14:59:15.206955",
     "exception": false,
     "start_time": "2025-05-26T14:57:36.002316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.2264\n",
      "Epoch 2/50, Loss: 0.0347\n",
      "Epoch 3/50, Loss: 0.0264\n",
      "Epoch 4/50, Loss: 0.0232\n",
      "Epoch 5/50, Loss: 0.0211\n",
      "Epoch 6/50, Loss: 0.0200\n",
      "Epoch 7/50, Loss: 0.0190\n",
      "Epoch 8/50, Loss: 0.0177\n",
      "Epoch 9/50, Loss: 0.0170\n",
      "Epoch 10/50, Loss: 0.0160\n",
      "Epoch 11/50, Loss: 0.0151\n",
      "Epoch 12/50, Loss: 0.0146\n",
      "Epoch 13/50, Loss: 0.0138\n",
      "Epoch 14/50, Loss: 0.0136\n",
      "Epoch 15/50, Loss: 0.0128\n",
      "Epoch 16/50, Loss: 0.0122\n",
      "Epoch 17/50, Loss: 0.0117\n",
      "Epoch 18/50, Loss: 0.0111\n",
      "Epoch 19/50, Loss: 0.0110\n",
      "Epoch 20/50, Loss: 0.0102\n",
      "Epoch 21/50, Loss: 0.0098\n",
      "Epoch 22/50, Loss: 0.0096\n",
      "Epoch 23/50, Loss: 0.0096\n",
      "Epoch 24/50, Loss: 0.0089\n",
      "Epoch 25/50, Loss: 0.0086\n",
      "Epoch 26/50, Loss: 0.0082\n",
      "Epoch 27/50, Loss: 0.0085\n",
      "Epoch 28/50, Loss: 0.0080\n",
      "Epoch 29/50, Loss: 0.0078\n",
      "Epoch 30/50, Loss: 0.0076\n",
      "Epoch 31/50, Loss: 0.0071\n",
      "Epoch 32/50, Loss: 0.0071\n",
      "Epoch 33/50, Loss: 0.0068\n",
      "Epoch 34/50, Loss: 0.0067\n",
      "Epoch 35/50, Loss: 0.0066\n",
      "Epoch 36/50, Loss: 0.0066\n",
      "Epoch 37/50, Loss: 0.0066\n",
      "Epoch 38/50, Loss: 0.0061\n",
      "Epoch 39/50, Loss: 0.0061\n",
      "Epoch 40/50, Loss: 0.0060\n",
      "Epoch 41/50, Loss: 0.0059\n",
      "Epoch 42/50, Loss: 0.0056\n",
      "Epoch 43/50, Loss: 0.0055\n",
      "Epoch 44/50, Loss: 0.0055\n",
      "Epoch 45/50, Loss: 0.0054\n",
      "Epoch 46/50, Loss: 0.0049\n",
      "Epoch 47/50, Loss: 0.0051\n",
      "Epoch 48/50, Loss: 0.0049\n",
      "Epoch 49/50, Loss: 0.0050\n",
      "Epoch 50/50, Loss: 0.0049\n",
      "Micro F1 Score: 0.9286\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       0.98      1.00      0.99       310\n",
      "         160       0.00      0.00      0.00         1\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       1.00      1.00      1.00         1\n",
      "         163       0.00      0.00      0.00         1\n",
      "         164       0.00      0.00      0.00         3\n",
      "         165       0.00      0.00      0.00         1\n",
      "         166       0.00      0.00      0.00         1\n",
      "         167       1.00      1.00      1.00         1\n",
      "         168       1.00      0.33      0.50         3\n",
      "         169       1.00      0.75      0.86         4\n",
      "         170       0.00      0.00      0.00         2\n",
      "         171       0.00      0.00      0.00         1\n",
      "         172       1.00      0.50      0.67         2\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.00      0.00      0.00         0\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       0.00      0.00      0.00         0\n",
      "         178       0.00      0.00      0.00         3\n",
      "         179       0.00      0.00      0.00         1\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         0\n",
      "         183       0.00      0.00      0.00         0\n",
      "         184       0.00      0.00      0.00         0\n",
      "         185       0.00      0.00      0.00         3\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         0\n",
      "         188       1.00      0.50      0.67         2\n",
      "         189       0.00      0.00      0.00         4\n",
      "         190       0.00      0.00      0.00         0\n",
      "         250       0.99      1.00      1.00       273\n",
      "         260       0.00      0.00      0.00         2\n",
      "         261       0.00      0.00      0.00         3\n",
      "         262       0.00      0.00      0.00         2\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         1\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       1.00      1.00      1.00         1\n",
      "         271       0.00      0.00      0.00         5\n",
      "         272       0.00      0.00      0.00         0\n",
      "         273       0.00      0.00      0.00         0\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       0.00      0.00      0.00         2\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         0\n",
      "         278       1.00      1.00      1.00         1\n",
      "         279       0.33      1.00      0.50         1\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       1.00      0.50      0.67         2\n",
      "         283       0.00      0.00      0.00         2\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         0\n",
      "         287       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         2\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         0\n",
      "         350       0.97      1.00      0.99       232\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         0\n",
      "         363       0.00      0.00      0.00         2\n",
      "         364       0.00      0.00      0.00         2\n",
      "         365       0.00      0.00      0.00         0\n",
      "         366       0.00      0.00      0.00         0\n",
      "         367       0.00      0.00      0.00         2\n",
      "         368       0.33      0.50      0.40         2\n",
      "         369       0.00      0.00      0.00         2\n",
      "         370       1.00      0.50      0.67         2\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.00      0.00      0.00         0\n",
      "         373       1.00      0.50      0.67         2\n",
      "         374       0.00      0.00      0.00         0\n",
      "         375       0.00      0.00      0.00         1\n",
      "         376       0.00      0.00      0.00         0\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         3\n",
      "         380       0.00      0.00      0.00         1\n",
      "         381       0.00      0.00      0.00         0\n",
      "         382       0.00      0.00      0.00         0\n",
      "         383       0.00      0.00      0.00         1\n",
      "         384       0.00      0.00      0.00         2\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       0.00      0.00      0.00         1\n",
      "         387       0.00      0.00      0.00         1\n",
      "         388       0.50      0.33      0.40         3\n",
      "         389       0.00      0.00      0.00         0\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       1.00      1.00      1.00       210\n",
      "         460       0.00      0.00      0.00         1\n",
      "         461       0.00      0.00      0.00         2\n",
      "         462       0.00      0.00      0.00         0\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.50      1.00      0.67         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         466       0.00      0.00      0.00         1\n",
      "         467       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         2\n",
      "         470       0.00      0.00      0.00         0\n",
      "         471       0.00      0.00      0.00         0\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         0\n",
      "         475       0.00      0.00      0.00         2\n",
      "         476       0.00      0.00      0.00         0\n",
      "         477       0.00      0.00      0.00         1\n",
      "         478       0.00      0.00      0.00         0\n",
      "         479       1.00      0.33      0.50         3\n",
      "         480       0.00      0.00      0.00         1\n",
      "         481       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.00      0.00      0.00         2\n",
      "         484       0.00      0.00      0.00         0\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       1.00      1.00      1.00         1\n",
      "         487       0.00      0.00      0.00         1\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       1.00      0.97      0.99       178\n",
      "         560       1.00      0.67      0.80         3\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         0\n",
      "         563       0.50      0.50      0.50         2\n",
      "         564       0.00      0.00      0.00         1\n",
      "         565       0.00      0.00      0.00         2\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       0.00      0.00      0.00         0\n",
      "         568       0.00      0.00      0.00         1\n",
      "         569       0.00      0.00      0.00         2\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         0\n",
      "         572       0.00      0.00      0.00         0\n",
      "         573       0.00      0.00      0.00         1\n",
      "         574       0.00      0.00      0.00         0\n",
      "         575       0.00      0.00      0.00         0\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         0\n",
      "         578       0.00      0.00      0.00         1\n",
      "         579       0.00      0.00      0.00         0\n",
      "         580       0.00      0.00      0.00         0\n",
      "         581       0.00      0.00      0.00         0\n",
      "         582       0.00      0.00      0.00         1\n",
      "         583       0.00      0.00      0.00         2\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         2\n",
      "         586       0.00      0.00      0.00         0\n",
      "         587       0.00      0.00      0.00         0\n",
      "         588       0.00      0.00      0.00         0\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         2\n",
      "         650       0.99      0.96      0.98       132\n",
      "         660       0.00      0.00      0.00         0\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         1\n",
      "         667       0.00      0.00      0.00         0\n",
      "         668       0.00      0.00      0.00         0\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         0\n",
      "         671       0.00      0.00      0.00         0\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         0\n",
      "         674       0.00      0.00      0.00         0\n",
      "         675       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         678       0.00      0.00      0.00         0\n",
      "         679       0.00      0.00      0.00         0\n",
      "         680       0.00      0.00      0.00         1\n",
      "         681       0.00      0.00      0.00         3\n",
      "         682       0.00      0.00      0.00         1\n",
      "         683       1.00      1.00      1.00         1\n",
      "         684       0.00      0.00      0.00         0\n",
      "         685       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         2\n",
      "         687       0.00      0.00      0.00         0\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         0\n",
      "         690       0.00      0.00      0.00         0\n",
      "         750       0.99      1.00      0.99        94\n",
      "         760       0.00      0.00      0.00         0\n",
      "         761       0.00      0.00      0.00         0\n",
      "         762       0.00      0.00      0.00         1\n",
      "         763       0.00      0.00      0.00         1\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         0\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         0\n",
      "         771       0.00      0.00      0.00         1\n",
      "         772       0.00      0.00      0.00         1\n",
      "         773       0.00      0.00      0.00         0\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         1\n",
      "         777       0.00      0.00      0.00         0\n",
      "         778       0.00      0.00      0.00         1\n",
      "         779       0.00      0.00      0.00         0\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         0\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         1\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         2\n",
      "         788       0.00      0.00      0.00         0\n",
      "         789       0.00      0.00      0.00         0\n",
      "         790       0.00      0.00      0.00         1\n",
      "         850       0.96      0.98      0.97        55\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         0\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         1\n",
      "         865       0.00      0.00      0.00         0\n",
      "         866       0.00      0.00      0.00         0\n",
      "         868       0.00      0.00      0.00         1\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         0\n",
      "         875       0.00      0.00      0.00         1\n",
      "         876       0.00      0.00      0.00         0\n",
      "         878       0.00      0.00      0.00         1\n",
      "         879       0.00      0.00      0.00         0\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         1\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         0\n",
      "         888       0.00      0.00      0.00         1\n",
      "         889       0.00      0.00      0.00         1\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       0.96      0.81      0.88        32\n",
      "         960       0.00      0.00      0.00         1\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         0\n",
      "         967       0.00      0.00      0.00         0\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         0\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         0\n",
      "         978       1.00      1.00      1.00         1\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         0\n",
      "         986       0.00      0.00      0.00         0\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.89      0.93      1706\n",
      "   macro avg       0.10      0.09      0.09      1706\n",
      "weighted avg       0.90      0.89      0.89      1706\n",
      " samples avg       0.84      0.78      0.81      1706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "amplitude_df = pd.read_csv(\"/kaggle/input/secound/all_signals_1.csv\", header=None)\n",
    "frequency_df = pd.read_csv(\"/kaggle/input/secound/all_frequencies_1.csv\", header=None)\n",
    "\n",
    "# Step 1: Prepare amplitude input (X)\n",
    "X = amplitude_df.iloc[:, 1:].values.astype(np.float32)\n",
    "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "# Step 2: Prepare frequency labels (y)\n",
    "y_raw = frequency_df.iloc[:, 1:].values\n",
    "label_sets = [list(set(row[row != 0])) for row in y_raw]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(label_sets)\n",
    "\n",
    "# Step 3: Tensor conversion\n",
    "X_tensor = torch.tensor(X).unsqueeze(1)  # Add channel dimension: (N, 1, T)\n",
    "y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# Step 4: Dataset and Dataloader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Step 5: Define CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_length, num_labels):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, padding=2),  # (B, 32, T)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # (B, 32, T/2)\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),  # (B, 64, T/2)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # (B, 64, T/4)\n",
    "        )\n",
    "        self.flatten_dim = (input_length // 4) * 64\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate CNN\n",
    "model = CNNClassifier(X.shape[1], y.shape[1])\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 6: Training\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/50, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Step 7: Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).int()\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "print(f\"Micro F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a31717",
   "metadata": {
    "papermill": {
     "duration": 0.00645,
     "end_time": "2025-05-26T14:59:15.220516",
     "exception": false,
     "start_time": "2025-05-26T14:59:15.214066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae20e993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T14:59:15.236081Z",
     "iopub.status.busy": "2025-05-26T14:59:15.235616Z",
     "iopub.status.idle": "2025-05-26T14:59:17.312198Z",
     "shell.execute_reply": "2025-05-26T14:59:17.311063Z"
    },
    "papermill": {
     "duration": 2.0861,
     "end_time": "2025-05-26T14:59:17.313843",
     "exception": false,
     "start_time": "2025-05-26T14:59:15.227743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.9286\n",
      "                                  Predicted  \\\n",
      "0                           [150, 250, 450]   \n",
      "1  [150, 250, 350, 450, 550, 650, 750, 950]   \n",
      "2                                        []   \n",
      "3            [150, 250, 350, 450, 550, 650]   \n",
      "4                      [150, 250, 350, 450]   \n",
      "5            [150, 250, 350, 450, 550, 650]   \n",
      "6                 [150, 250, 350, 450, 550]   \n",
      "7  [150, 250, 363, 450, 550, 650, 750, 850]   \n",
      "8                      [150, 250, 350, 550]   \n",
      "9            [150, 250, 350, 450, 550, 750]   \n",
      "\n",
      "                                            True  \n",
      "0                      [150, 250, 377, 450, 550]  \n",
      "1  [150, 250, 350, 450, 550, 650, 750, 878, 950]  \n",
      "2                                             []  \n",
      "3                 [150, 250, 350, 450, 550, 650]  \n",
      "4                           [150, 250, 350, 450]  \n",
      "5                 [150, 250, 350, 450, 550, 650]  \n",
      "6                      [150, 250, 350, 450, 550]  \n",
      "7       [150, 250, 364, 450, 550, 650, 750, 850]  \n",
      "8                      [150, 250, 350, 483, 550]  \n",
      "9            [150, 250, 350, 450, 550, 686, 750]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "comparison_rows = []\n",
    "\n",
    "threshold = 0.5  # You can tune this\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        probs = outputs  # Already sigmoid from model\n",
    "        preds = (probs > threshold).int()\n",
    "        \n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Record comparison\n",
    "        for p, t in zip(preds, labels):\n",
    "            pred_labels = [mlb.classes_[i] for i, val in enumerate(p) if val == 1]\n",
    "            true_labels = [mlb.classes_[i] for i, val in enumerate(t) if val == 1]\n",
    "            comparison_rows.append({\n",
    "                \"Predicted\": pred_labels,\n",
    "                \"True\": true_labels\n",
    "            })\n",
    "\n",
    "# Show F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "print(f\"Micro F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Convert comparisons to a DataFrame for easier viewing\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "print(comparison_df.head(10))  # Show first 10 predictions and ground truths\n",
    "\n",
    "\n",
    "comparison_df.to_csv(\"prediction_vs_ground_truth.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256fd0e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T14:59:17.329109Z",
     "iopub.status.busy": "2025-05-26T14:59:17.328790Z",
     "iopub.status.idle": "2025-05-26T14:59:17.335605Z",
     "shell.execute_reply": "2025-05-26T14:59:17.334662Z"
    },
    "papermill": {
     "duration": 0.016214,
     "end_time": "2025-05-26T14:59:17.337188",
     "exception": false,
     "start_time": "2025-05-26T14:59:17.320974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/prediction_vs_ground_truth.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Move CSV to /kaggle/working if it's not already there\n",
    "shutil.move(\"prediction_vs_ground_truth.csv\", \"/kaggle/working/prediction_vs_ground_truth.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15ab788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T14:59:17.352456Z",
     "iopub.status.busy": "2025-05-26T14:59:17.352159Z",
     "iopub.status.idle": "2025-05-26T14:59:17.356756Z",
     "shell.execute_reply": "2025-05-26T14:59:17.355845Z"
    },
    "papermill": {
     "duration": 0.01392,
     "end_time": "2025-05-26T14:59:17.358312",
     "exception": false,
     "start_time": "2025-05-26T14:59:17.344392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb900fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T14:59:17.374218Z",
     "iopub.status.busy": "2025-05-26T14:59:17.373941Z",
     "iopub.status.idle": "2025-05-26T15:00:43.467386Z",
     "shell.execute_reply": "2025-05-26T15:00:43.466559Z"
    },
    "papermill": {
     "duration": 86.105267,
     "end_time": "2025-05-26T15:00:43.471235",
     "exception": false,
     "start_time": "2025-05-26T14:59:17.365968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.0046\n",
      "Epoch 2/50, Loss: 0.0044\n",
      "Epoch 3/50, Loss: 0.0047\n",
      "Epoch 4/50, Loss: 0.0047\n",
      "Epoch 5/50, Loss: 0.0045\n",
      "Epoch 6/50, Loss: 0.0041\n",
      "Epoch 7/50, Loss: 0.0041\n",
      "Epoch 8/50, Loss: 0.0042\n",
      "Epoch 9/50, Loss: 0.0043\n",
      "Epoch 10/50, Loss: 0.0044\n",
      "Epoch 11/50, Loss: 0.0039\n",
      "Epoch 12/50, Loss: 0.0039\n",
      "Epoch 13/50, Loss: 0.0037\n",
      "Epoch 14/50, Loss: 0.0038\n",
      "Epoch 15/50, Loss: 0.0038\n",
      "Epoch 16/50, Loss: 0.0037\n",
      "Epoch 17/50, Loss: 0.0038\n",
      "Epoch 18/50, Loss: 0.0037\n",
      "Epoch 19/50, Loss: 0.0038\n",
      "Epoch 20/50, Loss: 0.0035\n",
      "Epoch 21/50, Loss: 0.0036\n",
      "Epoch 22/50, Loss: 0.0034\n",
      "Epoch 23/50, Loss: 0.0035\n",
      "Epoch 24/50, Loss: 0.0034\n",
      "Epoch 25/50, Loss: 0.0033\n",
      "Epoch 26/50, Loss: 0.0033\n",
      "Epoch 27/50, Loss: 0.0032\n",
      "Epoch 28/50, Loss: 0.0031\n",
      "Epoch 29/50, Loss: 0.0029\n",
      "Epoch 30/50, Loss: 0.0030\n",
      "Epoch 31/50, Loss: 0.0031\n",
      "Epoch 32/50, Loss: 0.0031\n",
      "Epoch 33/50, Loss: 0.0033\n",
      "Epoch 34/50, Loss: 0.0032\n",
      "Epoch 35/50, Loss: 0.0030\n",
      "Epoch 36/50, Loss: 0.0031\n",
      "Epoch 37/50, Loss: 0.0030\n",
      "Epoch 38/50, Loss: 0.0030\n",
      "Epoch 39/50, Loss: 0.0029\n",
      "Epoch 40/50, Loss: 0.0027\n",
      "Epoch 41/50, Loss: 0.0026\n",
      "Epoch 42/50, Loss: 0.0028\n",
      "Epoch 43/50, Loss: 0.0024\n",
      "Epoch 44/50, Loss: 0.0025\n",
      "Epoch 45/50, Loss: 0.0026\n",
      "Epoch 46/50, Loss: 0.0027\n",
      "Epoch 47/50, Loss: 0.0024\n",
      "Epoch 48/50, Loss: 0.0026\n",
      "Epoch 49/50, Loss: 0.0026\n",
      "Epoch 50/50, Loss: 0.0025\n",
      "\n",
      "‚è±Ô∏è Training Time (CNN): 85.96 seconds\n",
      "\n",
      "‚úÖ Micro F1 Score (CNN): 0.9286\n",
      "üéØ Match Accuracy (‚â§1% Label Error): 0.9900\n",
      "‚è±Ô∏è Training Time (CNN): 85.96 seconds\n",
      "\n",
      "üìã Classification Report (CNN):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       0.99      1.00      1.00       310\n",
      "         160       0.00      0.00      0.00         1\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       1.00      1.00      1.00         1\n",
      "         164       0.00      0.00      0.00         3\n",
      "         165       0.00      0.00      0.00         1\n",
      "         166       0.00      0.00      0.00         1\n",
      "         167       0.50      1.00      0.67         1\n",
      "         168       0.50      0.33      0.40         3\n",
      "         169       0.50      0.50      0.50         4\n",
      "         170       0.00      0.00      0.00         2\n",
      "         171       0.00      0.00      0.00         1\n",
      "         172       0.00      0.00      0.00         2\n",
      "         173       0.50      1.00      0.67         1\n",
      "         174       0.00      0.00      0.00         0\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       0.00      0.00      0.00         0\n",
      "         178       0.00      0.00      0.00         3\n",
      "         179       0.00      0.00      0.00         1\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       1.00      0.50      0.67         2\n",
      "         182       0.00      0.00      0.00         0\n",
      "         183       0.00      0.00      0.00         0\n",
      "         184       0.00      0.00      0.00         0\n",
      "         185       0.00      0.00      0.00         3\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         0\n",
      "         188       1.00      0.50      0.67         2\n",
      "         189       1.00      0.25      0.40         4\n",
      "         190       0.00      0.00      0.00         0\n",
      "         250       1.00      1.00      1.00       273\n",
      "         260       0.00      0.00      0.00         2\n",
      "         261       1.00      0.33      0.50         3\n",
      "         262       0.67      1.00      0.80         2\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         1\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       1.00      1.00      1.00         1\n",
      "         271       0.00      0.00      0.00         5\n",
      "         272       0.00      0.00      0.00         0\n",
      "         273       0.00      0.00      0.00         0\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       0.00      0.00      0.00         2\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         0\n",
      "         278       0.00      0.00      0.00         1\n",
      "         279       0.50      1.00      0.67         1\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       1.00      0.50      0.67         2\n",
      "         283       0.00      0.00      0.00         2\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         0\n",
      "         287       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         2\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         0\n",
      "         350       0.97      1.00      0.98       232\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       1.00      1.00      1.00         1\n",
      "         362       0.00      0.00      0.00         0\n",
      "         363       0.00      0.00      0.00         2\n",
      "         364       0.00      0.00      0.00         2\n",
      "         365       0.00      0.00      0.00         0\n",
      "         366       0.00      0.00      0.00         0\n",
      "         367       0.00      0.00      0.00         2\n",
      "         368       0.50      0.50      0.50         2\n",
      "         369       0.00      0.00      0.00         2\n",
      "         370       1.00      0.50      0.67         2\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.00      0.00      0.00         0\n",
      "         373       1.00      1.00      1.00         2\n",
      "         374       0.00      0.00      0.00         0\n",
      "         375       0.00      0.00      0.00         1\n",
      "         376       0.00      0.00      0.00         0\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         3\n",
      "         380       0.00      0.00      0.00         1\n",
      "         381       0.00      0.00      0.00         0\n",
      "         382       0.00      0.00      0.00         0\n",
      "         383       0.00      0.00      0.00         1\n",
      "         384       0.00      0.00      0.00         2\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       0.00      0.00      0.00         1\n",
      "         387       0.00      0.00      0.00         1\n",
      "         388       0.50      0.33      0.40         3\n",
      "         389       0.00      0.00      0.00         0\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       0.99      1.00      0.99       210\n",
      "         460       0.00      0.00      0.00         1\n",
      "         461       0.00      0.00      0.00         2\n",
      "         462       0.00      0.00      0.00         0\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.50      1.00      0.67         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         466       0.00      0.00      0.00         1\n",
      "         467       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         2\n",
      "         470       0.00      0.00      0.00         0\n",
      "         471       0.00      0.00      0.00         0\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         0\n",
      "         475       0.00      0.00      0.00         2\n",
      "         476       0.00      0.00      0.00         0\n",
      "         477       0.00      0.00      0.00         1\n",
      "         478       0.00      0.00      0.00         0\n",
      "         479       0.00      0.00      0.00         3\n",
      "         480       0.00      0.00      0.00         1\n",
      "         481       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.00      0.00      0.00         2\n",
      "         484       0.00      0.00      0.00         0\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       1.00      1.00      1.00         1\n",
      "         487       0.00      0.00      0.00         1\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       1.00      0.99      0.99       178\n",
      "         560       1.00      0.67      0.80         3\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         0\n",
      "         563       1.00      0.50      0.67         2\n",
      "         564       0.00      0.00      0.00         1\n",
      "         565       0.00      0.00      0.00         2\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       0.00      0.00      0.00         0\n",
      "         568       0.00      0.00      0.00         1\n",
      "         569       0.00      0.00      0.00         2\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         0\n",
      "         572       0.00      0.00      0.00         0\n",
      "         573       0.00      0.00      0.00         1\n",
      "         574       0.00      0.00      0.00         0\n",
      "         575       0.00      0.00      0.00         0\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         0\n",
      "         578       0.00      0.00      0.00         1\n",
      "         579       0.00      0.00      0.00         0\n",
      "         580       0.00      0.00      0.00         0\n",
      "         581       0.00      0.00      0.00         0\n",
      "         582       0.00      0.00      0.00         1\n",
      "         583       0.00      0.00      0.00         2\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         2\n",
      "         586       0.00      0.00      0.00         0\n",
      "         587       0.00      0.00      0.00         0\n",
      "         588       0.00      0.00      0.00         0\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         2\n",
      "         650       0.99      0.97      0.98       132\n",
      "         660       0.00      0.00      0.00         0\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         1\n",
      "         667       0.00      0.00      0.00         0\n",
      "         668       0.00      0.00      0.00         0\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         0\n",
      "         671       0.00      0.00      0.00         0\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         0\n",
      "         674       0.00      0.00      0.00         0\n",
      "         675       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         678       0.00      0.00      0.00         0\n",
      "         679       0.00      0.00      0.00         0\n",
      "         680       0.00      0.00      0.00         1\n",
      "         681       0.00      0.00      0.00         3\n",
      "         682       1.00      1.00      1.00         1\n",
      "         683       1.00      1.00      1.00         1\n",
      "         684       0.00      0.00      0.00         0\n",
      "         685       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         2\n",
      "         687       0.00      0.00      0.00         0\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         0\n",
      "         690       0.00      0.00      0.00         0\n",
      "         750       0.98      0.96      0.97        94\n",
      "         760       0.00      0.00      0.00         0\n",
      "         761       0.00      0.00      0.00         0\n",
      "         762       0.00      0.00      0.00         1\n",
      "         763       0.00      0.00      0.00         1\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         0\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         0\n",
      "         771       0.00      0.00      0.00         1\n",
      "         772       1.00      1.00      1.00         1\n",
      "         773       0.00      0.00      0.00         0\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         1\n",
      "         777       0.00      0.00      0.00         0\n",
      "         778       0.00      0.00      0.00         1\n",
      "         779       0.00      0.00      0.00         0\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         0\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         1\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         2\n",
      "         788       0.00      0.00      0.00         0\n",
      "         789       0.00      0.00      0.00         0\n",
      "         790       0.00      0.00      0.00         1\n",
      "         850       0.98      0.98      0.98        55\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         0\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         1\n",
      "         865       0.00      0.00      0.00         0\n",
      "         866       0.00      0.00      0.00         0\n",
      "         868       0.00      0.00      0.00         1\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         0\n",
      "         875       0.00      0.00      0.00         1\n",
      "         876       0.00      0.00      0.00         0\n",
      "         878       0.00      0.00      0.00         1\n",
      "         879       0.00      0.00      0.00         0\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         1\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         0\n",
      "         888       0.00      0.00      0.00         1\n",
      "         889       1.00      1.00      1.00         1\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       1.00      0.84      0.92        32\n",
      "         960       0.00      0.00      0.00         1\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         0\n",
      "         967       0.00      0.00      0.00         0\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         0\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         0\n",
      "         978       1.00      1.00      1.00         1\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         0\n",
      "         986       0.00      0.00      0.00         0\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.90      0.93      1706\n",
      "   macro avg       0.12      0.11      0.11      1706\n",
      "weighted avg       0.90      0.90      0.90      1706\n",
      " samples avg       0.84      0.79      0.81      1706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ----------------------\n",
    "# Training (CNN) ‚Äî Track Time\n",
    "# ----------------------\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/50, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_CNN = end_time - start_time\n",
    "print(f\"\\n‚è±Ô∏è Training Time (CNN): {training_time_CNN:.2f} seconds\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # 5% error tolerance: allow some label mismatches\n",
    "        total_labels = labels.size(1)\n",
    "        mismatches = (preds != labels).sum(dim=1)  # per-sample mismatches\n",
    "        allowed_errors = int(0.01*total_labels)\n",
    "\n",
    "        correct += (mismatches <= allowed_errors).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# ----------------------\n",
    "# Metrics\n",
    "# ----------------------\n",
    "f1_CNN = f1_score(all_labels, all_preds, average='micro')\n",
    "accuracy_CNN = correct / total\n",
    "\n",
    "print(f\"\\n‚úÖ Micro F1 Score (CNN): {f1_CNN:.4f}\")\n",
    "print(f\"üéØ Match Accuracy (‚â§1% Label Error): {accuracy_CNN:.4f}\")\n",
    "print(f\"‚è±Ô∏è Training Time (CNN): {training_time_CNN:.2f} seconds\")\n",
    "\n",
    "print(\"\\nüìã Classification Report (CNN):\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61d1239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:00:43.494023Z",
     "iopub.status.busy": "2025-05-26T15:00:43.493696Z",
     "iopub.status.idle": "2025-05-26T15:03:28.047612Z",
     "shell.execute_reply": "2025-05-26T15:03:28.046562Z"
    },
    "papermill": {
     "duration": 164.568923,
     "end_time": "2025-05-26T15:03:28.050721",
     "exception": false,
     "start_time": "2025-05-26T15:00:43.481798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.1581\n",
      "Epoch 2/50, Loss: 0.0419\n",
      "Epoch 3/50, Loss: 0.0273\n",
      "Epoch 4/50, Loss: 0.0253\n",
      "Epoch 5/50, Loss: 0.0242\n",
      "Epoch 6/50, Loss: 0.0229\n",
      "Epoch 7/50, Loss: 0.0224\n",
      "Epoch 8/50, Loss: 0.0217\n",
      "Epoch 9/50, Loss: 0.0213\n",
      "Epoch 10/50, Loss: 0.0204\n",
      "Epoch 11/50, Loss: 0.0194\n",
      "Epoch 12/50, Loss: 0.0187\n",
      "Epoch 13/50, Loss: 0.0179\n",
      "Epoch 14/50, Loss: 0.0173\n",
      "Epoch 15/50, Loss: 0.0164\n",
      "Epoch 16/50, Loss: 0.0153\n",
      "Epoch 17/50, Loss: 0.0146\n",
      "Epoch 18/50, Loss: 0.0135\n",
      "Epoch 19/50, Loss: 0.0129\n",
      "Epoch 20/50, Loss: 0.0118\n",
      "Epoch 21/50, Loss: 0.0112\n",
      "Epoch 22/50, Loss: 0.0106\n",
      "Epoch 23/50, Loss: 0.0099\n",
      "Epoch 24/50, Loss: 0.0096\n",
      "Epoch 25/50, Loss: 0.0089\n",
      "Epoch 26/50, Loss: 0.0088\n",
      "Epoch 27/50, Loss: 0.0080\n",
      "Epoch 28/50, Loss: 0.0078\n",
      "Epoch 29/50, Loss: 0.0074\n",
      "Epoch 30/50, Loss: 0.0071\n",
      "Epoch 31/50, Loss: 0.0065\n",
      "Epoch 32/50, Loss: 0.0066\n",
      "Epoch 33/50, Loss: 0.0064\n",
      "Epoch 34/50, Loss: 0.0060\n",
      "Epoch 35/50, Loss: 0.0058\n",
      "Epoch 36/50, Loss: 0.0058\n",
      "Epoch 37/50, Loss: 0.0053\n",
      "Epoch 38/50, Loss: 0.0052\n",
      "Epoch 39/50, Loss: 0.0051\n",
      "Epoch 40/50, Loss: 0.0048\n",
      "Epoch 41/50, Loss: 0.0048\n",
      "Epoch 42/50, Loss: 0.0046\n",
      "Epoch 43/50, Loss: 0.0043\n",
      "Epoch 44/50, Loss: 0.0043\n",
      "Epoch 45/50, Loss: 0.0040\n",
      "Epoch 46/50, Loss: 0.0039\n",
      "Epoch 47/50, Loss: 0.0038\n",
      "Epoch 48/50, Loss: 0.0038\n",
      "Epoch 49/50, Loss: 0.0035\n",
      "Epoch 50/50, Loss: 0.0036\n",
      "Micro F1 Score: 0.9204\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       1.00      1.00      1.00       313\n",
      "         160       0.00      0.00      0.00         2\n",
      "         161       0.00      0.00      0.00         0\n",
      "         162       0.00      0.00      0.00         0\n",
      "         163       0.00      0.00      0.00         1\n",
      "         164       0.00      0.00      0.00         1\n",
      "         165       0.00      0.00      0.00         0\n",
      "         166       0.00      0.00      0.00         3\n",
      "         167       0.00      0.00      0.00         1\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       0.00      0.00      0.00         3\n",
      "         170       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         0\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         3\n",
      "         174       0.00      0.00      0.00         1\n",
      "         175       0.00      0.00      0.00         2\n",
      "         176       0.00      0.00      0.00         2\n",
      "         177       1.00      1.00      1.00         1\n",
      "         178       0.00      0.00      0.00         1\n",
      "         179       0.00      0.00      0.00         2\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       1.00      0.33      0.50         3\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.00      0.00      0.00         3\n",
      "         185       0.00      0.00      0.00         3\n",
      "         186       0.00      0.00      0.00         3\n",
      "         187       0.00      0.00      0.00         1\n",
      "         188       1.00      0.25      0.40         4\n",
      "         189       1.00      0.33      0.50         3\n",
      "         190       0.00      0.00      0.00         0\n",
      "         250       1.00      1.00      1.00       296\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         0\n",
      "         262       1.00      1.00      1.00         1\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       1.00      1.00      1.00         1\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         0\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.00      0.00      0.00         2\n",
      "         272       0.00      0.00      0.00         0\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       1.00      0.33      0.50         3\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         1\n",
      "         279       1.00      0.50      0.67         2\n",
      "         280       0.00      0.00      0.00         0\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         1\n",
      "         285       0.33      1.00      0.50         1\n",
      "         286       0.00      0.00      0.00         0\n",
      "         287       0.50      0.50      0.50         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         1\n",
      "         350       1.00      0.99      0.99       246\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         0\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       0.00      0.00      0.00         2\n",
      "         366       0.00      0.00      0.00         1\n",
      "         367       0.00      0.00      0.00         2\n",
      "         368       0.00      0.00      0.00         1\n",
      "         369       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       0.00      0.00      0.00         2\n",
      "         372       0.00      0.00      0.00         0\n",
      "         373       1.00      0.50      0.67         2\n",
      "         374       0.00      0.00      0.00         0\n",
      "         375       0.00      0.00      0.00         0\n",
      "         376       0.00      0.00      0.00         2\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         0\n",
      "         380       0.00      0.00      0.00         0\n",
      "         381       0.00      0.00      0.00         2\n",
      "         382       0.00      0.00      0.00         1\n",
      "         383       0.00      0.00      0.00         0\n",
      "         384       0.00      0.00      0.00         5\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       0.00      0.00      0.00         0\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       1.00      0.50      0.67         2\n",
      "         389       0.00      0.00      0.00         0\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       1.00      0.99      0.99       213\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       0.00      0.00      0.00         0\n",
      "         462       0.00      0.00      0.00         2\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         2\n",
      "         465       0.00      0.00      0.00         0\n",
      "         466       0.00      0.00      0.00         0\n",
      "         467       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         1\n",
      "         470       0.00      0.00      0.00         0\n",
      "         471       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         2\n",
      "         475       0.00      0.00      0.00         2\n",
      "         476       0.00      0.00      0.00         2\n",
      "         477       0.00      0.00      0.00         1\n",
      "         478       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         0\n",
      "         480       0.00      0.00      0.00         0\n",
      "         481       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         2\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         0\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       0.00      0.00      0.00         0\n",
      "         487       0.00      0.00      0.00         1\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         2\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       0.99      0.95      0.97       175\n",
      "         560       1.00      0.67      0.80         3\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         0\n",
      "         563       0.00      0.00      0.00         0\n",
      "         564       0.00      0.00      0.00         0\n",
      "         565       0.00      0.00      0.00         1\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       1.00      0.50      0.67         2\n",
      "         568       0.00      0.00      0.00         0\n",
      "         569       0.00      0.00      0.00         3\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         0\n",
      "         572       0.00      0.00      0.00         0\n",
      "         573       0.00      0.00      0.00         0\n",
      "         574       0.00      0.00      0.00         1\n",
      "         575       0.00      0.00      0.00         2\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         2\n",
      "         578       0.00      0.00      0.00         2\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         2\n",
      "         582       0.00      0.00      0.00         0\n",
      "         583       0.00      0.00      0.00         0\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         0\n",
      "         586       1.00      1.00      1.00         1\n",
      "         587       0.00      0.00      0.00         1\n",
      "         588       0.00      0.00      0.00         1\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         2\n",
      "         650       0.99      0.98      0.98       133\n",
      "         660       0.00      0.00      0.00         0\n",
      "         661       1.00      0.50      0.67         2\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         0\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         0\n",
      "         667       0.00      0.00      0.00         0\n",
      "         668       1.00      0.50      0.67         2\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         1\n",
      "         671       0.00      0.00      0.00         0\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         0\n",
      "         674       0.00      0.00      0.00         0\n",
      "         675       0.00      0.00      0.00         2\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         0\n",
      "         678       0.00      0.00      0.00         0\n",
      "         679       0.00      0.00      0.00         1\n",
      "         680       0.00      0.00      0.00         0\n",
      "         681       0.00      0.00      0.00         0\n",
      "         682       0.00      0.00      0.00         0\n",
      "         683       0.00      0.00      0.00         2\n",
      "         684       0.00      0.00      0.00         1\n",
      "         685       0.00      0.00      0.00         1\n",
      "         686       0.00      0.00      0.00         1\n",
      "         687       0.00      0.00      0.00         1\n",
      "         688       0.00      0.00      0.00         0\n",
      "         689       1.00      0.33      0.50         3\n",
      "         690       0.00      0.00      0.00         1\n",
      "         750       1.00      0.94      0.97       108\n",
      "         760       0.00      0.00      0.00         1\n",
      "         761       0.00      0.00      0.00         0\n",
      "         762       0.00      0.00      0.00         0\n",
      "         763       0.00      0.00      0.00         0\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         0\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         0\n",
      "         770       0.00      0.00      0.00         0\n",
      "         771       0.00      0.00      0.00         0\n",
      "         772       1.00      1.00      1.00         1\n",
      "         773       0.00      0.00      0.00         0\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         1\n",
      "         777       0.00      0.00      0.00         0\n",
      "         778       1.00      0.50      0.67         2\n",
      "         779       0.00      0.00      0.00         0\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         0\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         0\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         2\n",
      "         788       0.00      0.00      0.00         0\n",
      "         789       0.00      0.00      0.00         0\n",
      "         790       0.00      0.00      0.00         1\n",
      "         850       0.97      0.84      0.90        80\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         1\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         0\n",
      "         865       0.00      0.00      0.00         1\n",
      "         866       0.00      0.00      0.00         1\n",
      "         868       0.00      0.00      0.00         0\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         1\n",
      "         875       0.00      0.00      0.00         1\n",
      "         876       0.00      0.00      0.00         1\n",
      "         878       0.00      0.00      0.00         0\n",
      "         879       0.00      0.00      0.00         0\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         0\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         1\n",
      "         888       0.00      0.00      0.00         0\n",
      "         889       0.50      0.50      0.50         2\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       1.00      0.78      0.88        41\n",
      "         960       0.00      0.00      0.00         1\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         2\n",
      "         967       0.00      0.00      0.00         0\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         1\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         0\n",
      "         978       0.00      0.00      0.00         0\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         0\n",
      "         986       0.00      0.00      0.00         1\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.87      0.92      1811\n",
      "   macro avg       0.11      0.08      0.09      1811\n",
      "weighted avg       0.90      0.87      0.89      1811\n",
      " samples avg       0.88      0.79      0.83      1811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "amplitude_df = pd.read_csv(\"/kaggle/input/secound/all_signals_1.csv\", header=None)\n",
    "frequency_df = pd.read_csv(\"/kaggle/input/secound/all_frequencies_1.csv\", header=None)\n",
    "\n",
    "# Step 1: Prepare amplitude input (X)\n",
    "X = amplitude_df.iloc[:, 1:].values.astype(np.float32)  # Skip timestamp\n",
    "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "# Step 2: Prepare frequency labels (y)\n",
    "y_raw = frequency_df.iloc[:, 1:].values\n",
    "label_sets = [list(set(row[row != 0])) for row in y_raw]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(label_sets)\n",
    "\n",
    "# Step 3: Tensor conversion\n",
    "X_tensor = torch.tensor(X).unsqueeze(1)  # Add channel dimension for SelfONN\n",
    "y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# Step 4: Dataset and Dataloader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Step 5: Define SelfONN Layer\n",
    "class SelfONNLayer1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, order=3, stride=1, padding=0):\n",
    "        super(SelfONNLayer1D, self).__init__()\n",
    "        self.order = order\n",
    "        self.weights = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(out_channels, in_channels, kernel_size))\n",
    "            for _ in range(order)\n",
    "        ])\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 0\n",
    "        for k in range(self.order):\n",
    "            out += nn.functional.conv1d(x ** (k + 1), self.weights[k], bias=None,\n",
    "                                        stride=self.stride, padding=self.padding)\n",
    "        out += self.bias.view(1, -1, 1)\n",
    "        return out\n",
    "\n",
    "# Step 6: Define SelfONN Classifier\n",
    "class SelfONNClassifier(nn.Module):\n",
    "    def __init__(self, input_length, num_labels, order=3):\n",
    "        super(SelfONNClassifier, self).__init__()\n",
    "        self.conv1 = SelfONNLayer1D(1, 32, kernel_size=5, order=order, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = SelfONNLayer1D(32, 64, kernel_size=3, order=order, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        flat_dim = (input_length // 4) * 64\n",
    "\n",
    "        self.fc1 = nn.Linear(flat_dim, 256)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "# Instantiate model\n",
    "input_length = X.shape[1]\n",
    "num_labels = y.shape[1]\n",
    "model = SelfONNClassifier(input_length=input_length, num_labels=num_labels, order=3)\n",
    "\n",
    "# Step 7: Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 8: Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/50, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Step 9: Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).int()\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "print(f\"Micro F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff628e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:03:28.079596Z",
     "iopub.status.busy": "2025-05-26T15:03:28.079168Z",
     "iopub.status.idle": "2025-05-26T15:06:22.071643Z",
     "shell.execute_reply": "2025-05-26T15:06:22.070680Z"
    },
    "papermill": {
     "duration": 174.010601,
     "end_time": "2025-05-26T15:06:22.074558",
     "exception": false,
     "start_time": "2025-05-26T15:03:28.063957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.0034\n",
      "Epoch 2/50, Loss: 0.0035\n",
      "Epoch 3/50, Loss: 0.0034\n",
      "Epoch 4/50, Loss: 0.0031\n",
      "Epoch 5/50, Loss: 0.0031\n",
      "Epoch 6/50, Loss: 0.0032\n",
      "Epoch 7/50, Loss: 0.0030\n",
      "Epoch 8/50, Loss: 0.0029\n",
      "Epoch 9/50, Loss: 0.0029\n",
      "Epoch 10/50, Loss: 0.0028\n",
      "Epoch 11/50, Loss: 0.0027\n",
      "Epoch 12/50, Loss: 0.0026\n",
      "Epoch 13/50, Loss: 0.0028\n",
      "Epoch 14/50, Loss: 0.0025\n",
      "Epoch 15/50, Loss: 0.0026\n",
      "Epoch 16/50, Loss: 0.0027\n",
      "Epoch 17/50, Loss: 0.0025\n",
      "Epoch 18/50, Loss: 0.0024\n",
      "Epoch 19/50, Loss: 0.0024\n",
      "Epoch 20/50, Loss: 0.0025\n",
      "Epoch 21/50, Loss: 0.0022\n",
      "Epoch 22/50, Loss: 0.0023\n",
      "Epoch 23/50, Loss: 0.0024\n",
      "Epoch 24/50, Loss: 0.0022\n",
      "Epoch 25/50, Loss: 0.0021\n",
      "Epoch 26/50, Loss: 0.0021\n",
      "Epoch 27/50, Loss: 0.0021\n",
      "Epoch 28/50, Loss: 0.0020\n",
      "Epoch 29/50, Loss: 0.0020\n",
      "Epoch 30/50, Loss: 0.0020\n",
      "Epoch 31/50, Loss: 0.0020\n",
      "Epoch 32/50, Loss: 0.0020\n",
      "Epoch 33/50, Loss: 0.0020\n",
      "Epoch 34/50, Loss: 0.0019\n",
      "Epoch 35/50, Loss: 0.0019\n",
      "Epoch 36/50, Loss: 0.0018\n",
      "Epoch 37/50, Loss: 0.0019\n",
      "Epoch 38/50, Loss: 0.0018\n",
      "Epoch 39/50, Loss: 0.0018\n",
      "Epoch 40/50, Loss: 0.0018\n",
      "Epoch 41/50, Loss: 0.0019\n",
      "Epoch 42/50, Loss: 0.0018\n",
      "Epoch 43/50, Loss: 0.0016\n",
      "Epoch 44/50, Loss: 0.0017\n",
      "Epoch 45/50, Loss: 0.0017\n",
      "Epoch 46/50, Loss: 0.0015\n",
      "Epoch 47/50, Loss: 0.0015\n",
      "Epoch 48/50, Loss: 0.0017\n",
      "Epoch 49/50, Loss: 0.0015\n",
      "Epoch 50/50, Loss: 0.0013\n",
      "\n",
      "‚è±Ô∏è Training Time (SONN): 173.74 seconds\n",
      "\n",
      "‚úÖ Micro F1 Score (SONN): 0.9227\n",
      "üéØ Match Accuracy (‚â§1% Label Error): 0.9725\n",
      "‚è±Ô∏è Training Time (SONN): 173.74 seconds\n",
      "\n",
      "üìã Classification Report (SONN):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       1.00      1.00      1.00       313\n",
      "         160       0.00      0.00      0.00         2\n",
      "         161       0.00      0.00      0.00         0\n",
      "         162       0.00      0.00      0.00         0\n",
      "         163       1.00      1.00      1.00         1\n",
      "         164       0.00      0.00      0.00         1\n",
      "         165       0.00      0.00      0.00         0\n",
      "         166       0.00      0.00      0.00         3\n",
      "         167       0.00      0.00      0.00         1\n",
      "         168       1.00      1.00      1.00         1\n",
      "         169       0.00      0.00      0.00         3\n",
      "         170       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         0\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         3\n",
      "         174       0.00      0.00      0.00         1\n",
      "         175       0.00      0.00      0.00         2\n",
      "         176       0.00      0.00      0.00         2\n",
      "         177       0.33      1.00      0.50         1\n",
      "         178       0.00      0.00      0.00         1\n",
      "         179       0.00      0.00      0.00         2\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       0.33      0.33      0.33         3\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.33      0.33      0.33         3\n",
      "         185       1.00      0.33      0.50         3\n",
      "         186       0.50      0.33      0.40         3\n",
      "         187       0.00      0.00      0.00         1\n",
      "         188       1.00      0.25      0.40         4\n",
      "         189       1.00      0.33      0.50         3\n",
      "         190       0.00      0.00      0.00         0\n",
      "         250       1.00      1.00      1.00       296\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         0\n",
      "         262       1.00      1.00      1.00         1\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       1.00      1.00      1.00         1\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         0\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.50      0.50      0.50         2\n",
      "         272       0.00      0.00      0.00         0\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       0.00      0.00      0.00         3\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         1\n",
      "         279       1.00      0.50      0.67         2\n",
      "         280       0.00      0.00      0.00         0\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         1\n",
      "         285       0.50      1.00      0.67         1\n",
      "         286       0.00      0.00      0.00         0\n",
      "         287       1.00      0.50      0.67         2\n",
      "         288       0.33      1.00      0.50         1\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         1\n",
      "         350       0.99      0.99      0.99       246\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         0\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       0.00      0.00      0.00         2\n",
      "         366       0.00      0.00      0.00         1\n",
      "         367       0.00      0.00      0.00         2\n",
      "         368       0.00      0.00      0.00         1\n",
      "         369       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       0.00      0.00      0.00         2\n",
      "         372       0.00      0.00      0.00         0\n",
      "         373       1.00      1.00      1.00         2\n",
      "         374       0.00      0.00      0.00         0\n",
      "         375       0.00      0.00      0.00         0\n",
      "         376       0.00      0.00      0.00         2\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         0\n",
      "         380       0.00      0.00      0.00         0\n",
      "         381       0.00      0.00      0.00         2\n",
      "         382       0.00      0.00      0.00         1\n",
      "         383       0.00      0.00      0.00         0\n",
      "         384       0.00      0.00      0.00         5\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       0.00      0.00      0.00         0\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       1.00      0.50      0.67         2\n",
      "         389       0.00      0.00      0.00         0\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       1.00      0.98      0.99       213\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       0.00      0.00      0.00         0\n",
      "         462       0.00      0.00      0.00         2\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       1.00      0.50      0.67         2\n",
      "         465       0.00      0.00      0.00         0\n",
      "         466       0.00      0.00      0.00         0\n",
      "         467       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         1\n",
      "         470       0.00      0.00      0.00         0\n",
      "         471       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         2\n",
      "         475       0.00      0.00      0.00         2\n",
      "         476       0.00      0.00      0.00         2\n",
      "         477       0.00      0.00      0.00         1\n",
      "         478       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         0\n",
      "         480       0.00      0.00      0.00         0\n",
      "         481       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         2\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         0\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       0.00      0.00      0.00         0\n",
      "         487       0.00      0.00      0.00         1\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         2\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       0.99      0.99      0.99       175\n",
      "         560       1.00      0.67      0.80         3\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         0\n",
      "         563       0.00      0.00      0.00         0\n",
      "         564       0.00      0.00      0.00         0\n",
      "         565       0.00      0.00      0.00         1\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       1.00      0.50      0.67         2\n",
      "         568       0.00      0.00      0.00         0\n",
      "         569       0.00      0.00      0.00         3\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         0\n",
      "         572       0.00      0.00      0.00         0\n",
      "         573       0.00      0.00      0.00         0\n",
      "         574       0.00      0.00      0.00         1\n",
      "         575       1.00      0.50      0.67         2\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         2\n",
      "         578       0.00      0.00      0.00         2\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         2\n",
      "         582       0.00      0.00      0.00         0\n",
      "         583       0.00      0.00      0.00         0\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         0\n",
      "         586       1.00      1.00      1.00         1\n",
      "         587       0.00      0.00      0.00         1\n",
      "         588       0.00      0.00      0.00         1\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         2\n",
      "         650       1.00      0.97      0.98       133\n",
      "         660       0.00      0.00      0.00         0\n",
      "         661       0.00      0.00      0.00         2\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         0\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         0\n",
      "         667       0.00      0.00      0.00         0\n",
      "         668       1.00      0.50      0.67         2\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         1\n",
      "         671       0.00      0.00      0.00         0\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         0\n",
      "         674       0.00      0.00      0.00         0\n",
      "         675       0.00      0.00      0.00         2\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         0\n",
      "         678       0.00      0.00      0.00         0\n",
      "         679       0.00      0.00      0.00         1\n",
      "         680       0.00      0.00      0.00         0\n",
      "         681       0.00      0.00      0.00         0\n",
      "         682       0.00      0.00      0.00         0\n",
      "         683       0.00      0.00      0.00         2\n",
      "         684       0.00      0.00      0.00         1\n",
      "         685       0.00      0.00      0.00         1\n",
      "         686       0.00      0.00      0.00         1\n",
      "         687       1.00      1.00      1.00         1\n",
      "         688       0.00      0.00      0.00         0\n",
      "         689       0.00      0.00      0.00         3\n",
      "         690       0.00      0.00      0.00         1\n",
      "         750       0.98      0.97      0.98       108\n",
      "         760       1.00      1.00      1.00         1\n",
      "         761       0.00      0.00      0.00         0\n",
      "         762       0.00      0.00      0.00         0\n",
      "         763       0.00      0.00      0.00         0\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         0\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         0\n",
      "         770       0.00      0.00      0.00         0\n",
      "         771       0.00      0.00      0.00         0\n",
      "         772       1.00      1.00      1.00         1\n",
      "         773       0.00      0.00      0.00         0\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         1\n",
      "         777       0.00      0.00      0.00         0\n",
      "         778       0.00      0.00      0.00         2\n",
      "         779       0.00      0.00      0.00         0\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         0\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         0\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         2\n",
      "         788       0.00      0.00      0.00         0\n",
      "         789       0.00      0.00      0.00         0\n",
      "         790       0.00      0.00      0.00         1\n",
      "         850       0.97      0.89      0.93        80\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         1\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         0\n",
      "         865       0.00      0.00      0.00         1\n",
      "         866       0.00      0.00      0.00         1\n",
      "         868       0.00      0.00      0.00         0\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         1\n",
      "         875       0.00      0.00      0.00         1\n",
      "         876       0.00      0.00      0.00         1\n",
      "         878       0.00      0.00      0.00         0\n",
      "         879       0.00      0.00      0.00         0\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         0\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         1\n",
      "         888       0.00      0.00      0.00         0\n",
      "         889       1.00      0.50      0.67         2\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       0.97      0.85      0.91        41\n",
      "         960       0.00      0.00      0.00         1\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         2\n",
      "         967       0.00      0.00      0.00         0\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         1\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         0\n",
      "         978       0.00      0.00      0.00         0\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         0\n",
      "         986       0.00      0.00      0.00         1\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.89      0.92      1811\n",
      "   macro avg       0.13      0.11      0.11      1811\n",
      "weighted avg       0.90      0.89      0.89      1811\n",
      " samples avg       0.87      0.81      0.83      1811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ----------------------\n",
    "# Training (SONN) ‚Äî Track Time\n",
    "# ----------------------\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/50, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_SONN = end_time - start_time\n",
    "print(f\"\\n‚è±Ô∏è Training Time (SONN): {training_time_SONN:.2f} seconds\")\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation (5% Label Error Tolerance)\n",
    "# ----------------------\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # 5% error tolerance: allow some label mismatches\n",
    "        total_labels = labels.size(1)\n",
    "        mismatches = (preds != labels).sum(dim=1)\n",
    "        allowed_errors = int(0.01 * total_labels)\n",
    "\n",
    "        correct += (mismatches <= allowed_errors).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# ----------------------\n",
    "# Metrics\n",
    "# ----------------------\n",
    "f1_SONN = f1_score(all_labels, all_preds, average='micro')\n",
    "accuracy_SONN = correct / total\n",
    "\n",
    "print(f\"\\n‚úÖ Micro F1 Score (SONN): {f1_SONN:.4f}\")\n",
    "print(f\"üéØ Match Accuracy (‚â§1% Label Error): {accuracy_SONN:.4f}\")\n",
    "print(f\"‚è±Ô∏è Training Time (SONN): {training_time_SONN:.2f} seconds\")\n",
    "\n",
    "print(\"\\nüìã Classification Report (SONN):\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daeb2e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:06:22.108364Z",
     "iopub.status.busy": "2025-05-26T15:06:22.108064Z",
     "iopub.status.idle": "2025-05-26T15:06:22.112515Z",
     "shell.execute_reply": "2025-05-26T15:06:22.111535Z"
    },
    "papermill": {
     "duration": 0.023762,
     "end_time": "2025-05-26T15:06:22.114089",
     "exception": false,
     "start_time": "2025-05-26T15:06:22.090327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install snntorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beeec394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:06:22.149189Z",
     "iopub.status.busy": "2025-05-26T15:06:22.148883Z",
     "iopub.status.idle": "2025-05-26T15:06:22.155493Z",
     "shell.execute_reply": "2025-05-26T15:06:22.154400Z"
    },
    "papermill": {
     "duration": 0.026839,
     "end_time": "2025-05-26T15:06:22.157111",
     "exception": false,
     "start_time": "2025-05-26T15:06:22.130272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.metrics import f1_score, classification_report\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import snntorch as snn\n",
    "# from snntorch import surrogate\n",
    "# from snntorch import functional as SF\n",
    "\n",
    "# # Load and normalize data\n",
    "# amplitude_df = pd.read_csv(\"/kaggle/input/amplifreq/all_signals.csv\", header=None)\n",
    "# frequency_df = pd.read_csv(\"/kaggle/input/amplifreq/all_frequencies.csv\", header=None)\n",
    "\n",
    "# X = amplitude_df.iloc[:, 1:].values.astype(np.float32)\n",
    "# X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
    "# y_raw = frequency_df.iloc[:, 1:].values\n",
    "# label_sets = [list(set(row[row != 0])) for row in y_raw]\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# y = mlb.fit_transform(label_sets)\n",
    "\n",
    "# X_tensor = torch.tensor(X).unsqueeze(1)  # (B, C=1, T)\n",
    "# y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# # SNN Classifier\n",
    "# class SNNClassifier(nn.Module):\n",
    "#     def __init__(self, input_length, num_labels, time_window=20):\n",
    "#         super().__init__()\n",
    "#         self.time_window = time_window\n",
    "#         self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)\n",
    "#         self.lif1 = snn.Leaky(beta=0.95, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "#         self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "#         self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.lif2 = snn.Leaky(beta=0.95, spike_grad=surrogate.fast_sigmoid())\n",
    "#         self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "#         flat_dim = (input_length // 4) * 64\n",
    "#         self.fc1 = nn.Linear(flat_dim, 256)\n",
    "#         self.lif3 = snn.Leaky(beta=0.95, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "#         self.fc2 = nn.Linear(256, num_labels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         spk_rec = []\n",
    "#         mem1 = mem2 = mem3 = 0\n",
    "\n",
    "#         for step in range(self.time_window):\n",
    "#             cur_input = x + torch.rand_like(x) * 0.01  # small noise simulating input variation\n",
    "\n",
    "#             x1 = self.conv1(cur_input)\n",
    "#             spk1, mem1 = self.lif1(x1, mem1)\n",
    "#             x1 = self.pool1(spk1)\n",
    "\n",
    "#             x2 = self.conv2(x1)\n",
    "#             spk2, mem2 = self.lif2(x2, mem2)\n",
    "#             x2 = self.pool2(spk2)\n",
    "\n",
    "#             x2 = x2.view(x2.size(0), -1)\n",
    "#             x3 = self.fc1(x2)\n",
    "#             spk3, mem3 = self.lif3(x3, mem3)\n",
    "\n",
    "#             out = self.fc2(spk3)\n",
    "#             spk_rec.append(out)\n",
    "\n",
    "#         spk_rec = torch.stack(spk_rec, dim=0)  # (time, batch, labels)\n",
    "#         out_rate = spk_rec.mean(dim=0)  # firing rate\n",
    "#         return torch.sigmoid(out_rate)\n",
    "\n",
    "# # Instantiate model\n",
    "# input_length = X.shape[1]\n",
    "# num_labels = y.shape[1]\n",
    "# model = SNNClassifier(input_length=input_length, num_labels=num_labels, time_window=20)\n",
    "\n",
    "# # Training Setup\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch in range(20):  # Reduce epochs due to longer simulation\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in val_loader:\n",
    "#         outputs = model(inputs)\n",
    "#         preds = (outputs > 0.5).int()\n",
    "#         all_preds.extend(preds.numpy())\n",
    "#         all_labels.extend(labels.numpy())\n",
    "\n",
    "# f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "# print(f\"Micro F1 Score: {f1:.4f}\")\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b9e533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:06:22.191717Z",
     "iopub.status.busy": "2025-05-26T15:06:22.191330Z",
     "iopub.status.idle": "2025-05-26T15:13:05.151806Z",
     "shell.execute_reply": "2025-05-26T15:13:05.150682Z"
    },
    "papermill": {
     "duration": 402.980213,
     "end_time": "2025-05-26T15:13:05.154193",
     "exception": false,
     "start_time": "2025-05-26T15:06:22.173980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0883\n",
      "Epoch 2, Loss: 0.0233\n",
      "Epoch 3, Loss: 0.0183\n",
      "Epoch 4, Loss: 0.0157\n",
      "Epoch 5, Loss: 0.0135\n",
      "Epoch 6, Loss: 0.0114\n",
      "Epoch 7, Loss: 0.0097\n",
      "Epoch 8, Loss: 0.0082\n",
      "Epoch 9, Loss: 0.0070\n",
      "Epoch 10, Loss: 0.0059\n",
      "Epoch 11, Loss: 0.0050\n",
      "Epoch 12, Loss: 0.0043\n",
      "Epoch 13, Loss: 0.0038\n",
      "Epoch 14, Loss: 0.0033\n",
      "Epoch 15, Loss: 0.0027\n",
      "Epoch 16, Loss: 0.0026\n",
      "Epoch 17, Loss: 0.0022\n",
      "Epoch 18, Loss: 0.0020\n",
      "Epoch 19, Loss: 0.0019\n",
      "Epoch 20, Loss: 0.0016\n",
      "Epoch 21, Loss: 0.0016\n",
      "Epoch 22, Loss: 0.0014\n",
      "Epoch 23, Loss: 0.0013\n",
      "Epoch 24, Loss: 0.0012\n",
      "Epoch 25, Loss: 0.0011\n",
      "Epoch 26, Loss: 0.0011\n",
      "Epoch 27, Loss: 0.0010\n",
      "Epoch 28, Loss: 0.0009\n",
      "Epoch 29, Loss: 0.0009\n",
      "Epoch 30, Loss: 0.0008\n",
      "Epoch 31, Loss: 0.0008\n",
      "Epoch 32, Loss: 0.0008\n",
      "Epoch 33, Loss: 0.0007\n",
      "Epoch 34, Loss: 0.0007\n",
      "Epoch 35, Loss: 0.0007\n",
      "Epoch 36, Loss: 0.0006\n",
      "Epoch 37, Loss: 0.0006\n",
      "Epoch 38, Loss: 0.0006\n",
      "Epoch 39, Loss: 0.0006\n",
      "Epoch 40, Loss: 0.0005\n",
      "Epoch 41, Loss: 0.0006\n",
      "Epoch 42, Loss: 0.0005\n",
      "Epoch 43, Loss: 0.0004\n",
      "Epoch 44, Loss: 0.0004\n",
      "Epoch 45, Loss: 0.0004\n",
      "Epoch 46, Loss: 0.0005\n",
      "Epoch 47, Loss: 0.0004\n",
      "Epoch 48, Loss: 0.0005\n",
      "Epoch 49, Loss: 0.0004\n",
      "Epoch 50, Loss: 0.0004\n",
      "\n",
      "Micro F1 Score: 0.8967\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       0.98      1.00      0.99       315\n",
      "         160       0.00      0.00      0.00         0\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       0.00      0.00      0.00         0\n",
      "         164       0.00      0.00      0.00         1\n",
      "         165       0.00      0.00      0.00         4\n",
      "         166       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         1\n",
      "         168       1.00      0.50      0.67         2\n",
      "         169       0.00      0.00      0.00         2\n",
      "         170       0.00      0.00      0.00         0\n",
      "         171       0.00      0.00      0.00         2\n",
      "         172       0.00      0.00      0.00         3\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.00      0.00      0.00         0\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       0.00      0.00      0.00         3\n",
      "         178       0.00      0.00      0.00         4\n",
      "         179       0.00      0.00      0.00         0\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00         1\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.00      0.00      0.00         1\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         0\n",
      "         188       0.00      0.00      0.00         2\n",
      "         189       0.00      0.00      0.00         2\n",
      "         190       0.00      0.00      0.00         0\n",
      "         250       0.98      1.00      0.99       289\n",
      "         260       0.00      0.00      0.00         2\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00         0\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         3\n",
      "         265       0.00      0.00      0.00         1\n",
      "         266       0.00      0.00      0.00         0\n",
      "         267       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         2\n",
      "         270       0.00      0.00      0.00         0\n",
      "         271       1.00      0.50      0.67         2\n",
      "         272       0.00      0.00      0.00         1\n",
      "         273       0.00      0.00      0.00         2\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       0.00      0.00      0.00         1\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         0\n",
      "         278       0.00      0.00      0.00         0\n",
      "         279       0.00      0.00      0.00         2\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         1\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         0\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         0\n",
      "         350       0.96      0.98      0.97       247\n",
      "         360       0.00      0.00      0.00         0\n",
      "         361       0.00      0.00      0.00         0\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         2\n",
      "         365       0.00      0.00      0.00         0\n",
      "         366       0.00      0.00      0.00         0\n",
      "         367       0.00      0.00      0.00         0\n",
      "         368       0.00      0.00      0.00         1\n",
      "         369       0.00      0.00      0.00         2\n",
      "         370       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         0\n",
      "         372       0.00      0.00      0.00         0\n",
      "         373       0.00      0.00      0.00         3\n",
      "         374       0.00      0.00      0.00         1\n",
      "         375       0.00      0.00      0.00         1\n",
      "         376       0.00      0.00      0.00         0\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.00      0.00      0.00         2\n",
      "         381       0.00      0.00      0.00         0\n",
      "         382       0.00      0.00      0.00         2\n",
      "         383       0.00      0.00      0.00         1\n",
      "         384       0.00      0.00      0.00         1\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       0.00      0.00      0.00         0\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       0.00      0.00      0.00         5\n",
      "         389       0.00      0.00      0.00         0\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       0.97      0.93      0.95       215\n",
      "         460       0.00      0.00      0.00         0\n",
      "         461       0.00      0.00      0.00         2\n",
      "         462       0.00      0.00      0.00         1\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         0\n",
      "         465       0.00      0.00      0.00         0\n",
      "         466       0.00      0.00      0.00         0\n",
      "         467       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         3\n",
      "         470       0.00      0.00      0.00         1\n",
      "         471       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         1\n",
      "         475       0.00      0.00      0.00         1\n",
      "         476       0.00      0.00      0.00         0\n",
      "         477       0.00      0.00      0.00         0\n",
      "         478       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         0\n",
      "         480       0.00      0.00      0.00         4\n",
      "         481       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         2\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         3\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         0\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         2\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       0.98      0.93      0.96       173\n",
      "         560       0.00      0.00      0.00         1\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         1\n",
      "         563       0.00      0.00      0.00         1\n",
      "         564       0.00      0.00      0.00         2\n",
      "         565       0.00      0.00      0.00         0\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       0.00      0.00      0.00         2\n",
      "         568       0.00      0.00      0.00         1\n",
      "         569       0.00      0.00      0.00         0\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         4\n",
      "         572       0.00      0.00      0.00         2\n",
      "         573       0.00      0.00      0.00         2\n",
      "         574       0.00      0.00      0.00         2\n",
      "         575       0.00      0.00      0.00         5\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         1\n",
      "         578       0.00      0.00      0.00         0\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         2\n",
      "         582       0.00      0.00      0.00         1\n",
      "         583       0.00      0.00      0.00         2\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         0\n",
      "         586       0.00      0.00      0.00         1\n",
      "         587       0.00      0.00      0.00         1\n",
      "         588       0.00      0.00      0.00         0\n",
      "         589       0.00      0.00      0.00         0\n",
      "         590       0.00      0.00      0.00         0\n",
      "         650       0.99      0.83      0.90       145\n",
      "         660       0.00      0.00      0.00         1\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         0\n",
      "         667       0.00      0.00      0.00         0\n",
      "         668       0.00      0.00      0.00         1\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         0\n",
      "         671       0.00      0.00      0.00         1\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         2\n",
      "         674       0.00      0.00      0.00         0\n",
      "         675       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         0\n",
      "         678       0.00      0.00      0.00         0\n",
      "         679       0.00      0.00      0.00         0\n",
      "         680       0.00      0.00      0.00         1\n",
      "         681       0.00      0.00      0.00         1\n",
      "         682       0.00      0.00      0.00         0\n",
      "         683       0.00      0.00      0.00         1\n",
      "         684       0.00      0.00      0.00         2\n",
      "         685       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         1\n",
      "         687       0.00      0.00      0.00         2\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         1\n",
      "         690       0.00      0.00      0.00         0\n",
      "         750       0.95      0.87      0.91        99\n",
      "         760       0.00      0.00      0.00         0\n",
      "         761       0.00      0.00      0.00         1\n",
      "         762       0.00      0.00      0.00         3\n",
      "         763       0.00      0.00      0.00         0\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         1\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         1\n",
      "         771       0.00      0.00      0.00         1\n",
      "         772       0.00      0.00      0.00         0\n",
      "         773       0.00      0.00      0.00         1\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         0\n",
      "         777       0.00      0.00      0.00         1\n",
      "         778       0.00      0.00      0.00         2\n",
      "         779       0.00      0.00      0.00         1\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         1\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         1\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         1\n",
      "         788       0.00      0.00      0.00         0\n",
      "         789       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         0\n",
      "         850       0.97      0.76      0.85        74\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         0\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         0\n",
      "         865       0.00      0.00      0.00         1\n",
      "         866       0.00      0.00      0.00         0\n",
      "         868       0.00      0.00      0.00         1\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         1\n",
      "         875       0.00      0.00      0.00         0\n",
      "         876       0.00      0.00      0.00         0\n",
      "         878       0.00      0.00      0.00         1\n",
      "         879       0.00      0.00      0.00         0\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         0\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         0\n",
      "         888       0.00      0.00      0.00         1\n",
      "         889       0.00      0.00      0.00         0\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       0.96      0.61      0.75        36\n",
      "         960       0.00      0.00      0.00         0\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         1\n",
      "         967       0.00      0.00      0.00         1\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         0\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         1\n",
      "         978       0.00      0.00      0.00         0\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         0\n",
      "         986       0.00      0.00      0.00         1\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.83      0.90      1797\n",
      "   macro avg       0.04      0.03      0.04      1797\n",
      "weighted avg       0.87      0.83      0.85      1797\n",
      " samples avg       0.86      0.75      0.79      1797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------\n",
    "# Load and preprocess data\n",
    "# ----------------------\n",
    "amplitude_df = pd.read_csv(\"/kaggle/input/secound/all_signals_1.csv\", header=None)\n",
    "frequency_df = pd.read_csv(\"/kaggle/input/secound/all_frequencies_1.csv\", header=None)\n",
    "\n",
    "X = amplitude_df.iloc[:, 1:].values.astype(np.float32)\n",
    "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "y_raw = frequency_df.iloc[:, 1:].values\n",
    "label_sets = [list(set(row[row != 0])) for row in y_raw]\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(label_sets)\n",
    "\n",
    "X_tensor = torch.tensor(X).unsqueeze(1)  # shape: [N, 1, T]\n",
    "y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# Optionally reduce dataset size for memory reasons\n",
    "X_tensor = X_tensor[:2000]\n",
    "y_tensor = y_tensor[:2000]\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# ----------------------\n",
    "# Define Spiking Neuron\n",
    "# ----------------------\n",
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, threshold=1.0, decay=0.9):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, channels, time]\n",
    "        batch_size, channels, time = x.size()\n",
    "        mem = torch.zeros(batch_size, channels, device=x.device)\n",
    "        spikes = []\n",
    "\n",
    "        for t in range(time):\n",
    "            input_t = x[:, :, t]  # shape: [batch, channels]\n",
    "            mem = self.decay * mem + input_t\n",
    "            spike = (mem >= self.threshold).float()\n",
    "            mem = mem * (1 - spike)  # reset after spike\n",
    "            spikes.append(spike.unsqueeze(-1))  # shape: [batch, channels, 1]\n",
    "\n",
    "        return torch.cat(spikes, dim=-1)  # shape: [batch, channels, time]\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# SNN Model Definition\n",
    "# ----------------------\n",
    "class SNNClassifier(nn.Module):\n",
    "    def __init__(self, input_length, num_labels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)\n",
    "        self.lif1 = LIFNeuron()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.lif2 = LIFNeuron()\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        flat_dim = (input_length // 4) * 64\n",
    "        self.fc1 = nn.Linear(flat_dim, 256)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.lif1(self.conv1(x)))\n",
    "        x = self.pool2(self.lif2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "# ----------------------\n",
    "# Train & Evaluate\n",
    "# ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_length = X_tensor.shape[2]\n",
    "num_labels = y_tensor.shape[1]\n",
    "model = SNNClassifier(input_length, num_labels).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):  # Fewer epochs for memory safety\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation\n",
    "# ----------------------\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs.cpu() > 0.5).int()\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "print(f\"\\nMicro F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3460d4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:13:05.194788Z",
     "iopub.status.busy": "2025-05-26T15:13:05.194418Z",
     "iopub.status.idle": "2025-05-26T15:13:05.200188Z",
     "shell.execute_reply": "2025-05-26T15:13:05.199292Z"
    },
    "papermill": {
     "duration": 0.028156,
     "end_time": "2025-05-26T15:13:05.201971",
     "exception": false,
     "start_time": "2025-05-26T15:13:05.173815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 801)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amplitude_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54565ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:13:05.243186Z",
     "iopub.status.busy": "2025-05-26T15:13:05.242856Z",
     "iopub.status.idle": "2025-05-26T15:19:17.181176Z",
     "shell.execute_reply": "2025-05-26T15:19:17.180293Z"
    },
    "papermill": {
     "duration": 371.962983,
     "end_time": "2025-05-26T15:19:17.184596",
     "exception": false,
     "start_time": "2025-05-26T15:13:05.221613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.0004\n",
      "Epoch 2/50, Loss: 0.0004\n",
      "Epoch 3/50, Loss: 0.0004\n",
      "Epoch 4/50, Loss: 0.0004\n",
      "Epoch 5/50, Loss: 0.0003\n",
      "Epoch 6/50, Loss: 0.0003\n",
      "Epoch 7/50, Loss: 0.0003\n",
      "Epoch 8/50, Loss: 0.0004\n",
      "Epoch 9/50, Loss: 0.0003\n",
      "Epoch 10/50, Loss: 0.0003\n",
      "Epoch 11/50, Loss: 0.0003\n",
      "Epoch 12/50, Loss: 0.0003\n",
      "Epoch 13/50, Loss: 0.0003\n",
      "Epoch 14/50, Loss: 0.0003\n",
      "Epoch 15/50, Loss: 0.0003\n",
      "Epoch 16/50, Loss: 0.0003\n",
      "Epoch 17/50, Loss: 0.0003\n",
      "Epoch 18/50, Loss: 0.0003\n",
      "Epoch 19/50, Loss: 0.0003\n",
      "Epoch 20/50, Loss: 0.0003\n",
      "Epoch 21/50, Loss: 0.0003\n",
      "Epoch 22/50, Loss: 0.0003\n",
      "Epoch 23/50, Loss: 0.0003\n",
      "Epoch 24/50, Loss: 0.0003\n",
      "Epoch 25/50, Loss: 0.0003\n",
      "Epoch 26/50, Loss: 0.0003\n",
      "Epoch 27/50, Loss: 0.0003\n",
      "Epoch 28/50, Loss: 0.0003\n",
      "Epoch 29/50, Loss: 0.0003\n",
      "Epoch 30/50, Loss: 0.0002\n",
      "Epoch 31/50, Loss: 0.0002\n",
      "Epoch 32/50, Loss: 0.0003\n",
      "Epoch 33/50, Loss: 0.0003\n",
      "Epoch 34/50, Loss: 0.0003\n",
      "Epoch 35/50, Loss: 0.0002\n",
      "Epoch 36/50, Loss: 0.0003\n",
      "Epoch 37/50, Loss: 0.0002\n",
      "Epoch 38/50, Loss: 0.0002\n",
      "Epoch 39/50, Loss: 0.0002\n",
      "Epoch 40/50, Loss: 0.0002\n",
      "Epoch 41/50, Loss: 0.0002\n",
      "Epoch 42/50, Loss: 0.0002\n",
      "Epoch 43/50, Loss: 0.0003\n",
      "Epoch 44/50, Loss: 0.0003\n",
      "Epoch 45/50, Loss: 0.0003\n",
      "Epoch 46/50, Loss: 0.0002\n",
      "Epoch 47/50, Loss: 0.0003\n",
      "Epoch 48/50, Loss: 0.0003\n",
      "Epoch 49/50, Loss: 0.0002\n",
      "Epoch 50/50, Loss: 0.0002\n",
      "\n",
      "‚è±Ô∏è Training Time: 370.69 seconds\n",
      "\n",
      "‚úÖ Micro F1 Score: 0.8995\n",
      "üéØ Match Accuracy with ‚â§1% Label Error: 0.9450\n",
      "‚è±Ô∏è Training Time: 370.69 seconds\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       0.99      1.00      0.99       315\n",
      "         160       0.00      0.00      0.00         0\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       0.00      0.00      0.00         0\n",
      "         164       0.00      0.00      0.00         1\n",
      "         165       0.00      0.00      0.00         4\n",
      "         166       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         1\n",
      "         168       1.00      0.50      0.67         2\n",
      "         169       0.00      0.00      0.00         2\n",
      "         170       0.00      0.00      0.00         0\n",
      "         171       0.00      0.00      0.00         2\n",
      "         172       0.00      0.00      0.00         3\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.00      0.00      0.00         0\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       0.00      0.00      0.00         3\n",
      "         178       1.00      0.25      0.40         4\n",
      "         179       0.00      0.00      0.00         0\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00         1\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.00      0.00      0.00         1\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         0\n",
      "         188       0.00      0.00      0.00         2\n",
      "         189       0.00      0.00      0.00         2\n",
      "         190       0.00      0.00      0.00         0\n",
      "         250       0.98      1.00      0.99       289\n",
      "         260       0.00      0.00      0.00         2\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00         0\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         3\n",
      "         265       0.00      0.00      0.00         1\n",
      "         266       0.00      0.00      0.00         0\n",
      "         267       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         2\n",
      "         270       0.00      0.00      0.00         0\n",
      "         271       1.00      0.50      0.67         2\n",
      "         272       0.00      0.00      0.00         1\n",
      "         273       0.00      0.00      0.00         2\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       0.00      0.00      0.00         1\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         0\n",
      "         278       0.00      0.00      0.00         0\n",
      "         279       1.00      0.50      0.67         2\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         1\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         0\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         0\n",
      "         350       0.99      0.98      0.99       247\n",
      "         360       0.00      0.00      0.00         0\n",
      "         361       0.00      0.00      0.00         0\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         2\n",
      "         365       0.00      0.00      0.00         0\n",
      "         366       0.00      0.00      0.00         0\n",
      "         367       0.00      0.00      0.00         0\n",
      "         368       0.00      0.00      0.00         1\n",
      "         369       0.00      0.00      0.00         2\n",
      "         370       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         0\n",
      "         372       0.00      0.00      0.00         0\n",
      "         373       0.00      0.00      0.00         3\n",
      "         374       0.00      0.00      0.00         1\n",
      "         375       0.00      0.00      0.00         1\n",
      "         376       0.00      0.00      0.00         0\n",
      "         377       0.00      0.00      0.00         3\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.00      0.00      0.00         2\n",
      "         381       0.00      0.00      0.00         0\n",
      "         382       0.00      0.00      0.00         2\n",
      "         383       0.00      0.00      0.00         1\n",
      "         384       0.00      0.00      0.00         1\n",
      "         385       0.00      0.00      0.00         1\n",
      "         386       0.00      0.00      0.00         0\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       0.00      0.00      0.00         5\n",
      "         389       0.00      0.00      0.00         0\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       0.97      0.95      0.96       215\n",
      "         460       0.00      0.00      0.00         0\n",
      "         461       0.00      0.00      0.00         2\n",
      "         462       0.00      0.00      0.00         1\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         0\n",
      "         465       0.00      0.00      0.00         0\n",
      "         466       0.00      0.00      0.00         0\n",
      "         467       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         3\n",
      "         470       0.00      0.00      0.00         1\n",
      "         471       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         1\n",
      "         475       0.00      0.00      0.00         1\n",
      "         476       0.00      0.00      0.00         0\n",
      "         477       0.00      0.00      0.00         0\n",
      "         478       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         0\n",
      "         480       0.00      0.00      0.00         4\n",
      "         481       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         2\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         3\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         0\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         2\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       0.93      0.95      0.94       173\n",
      "         560       0.00      0.00      0.00         1\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         1\n",
      "         563       0.00      0.00      0.00         1\n",
      "         564       0.00      0.00      0.00         2\n",
      "         565       0.00      0.00      0.00         0\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       0.00      0.00      0.00         2\n",
      "         568       0.00      0.00      0.00         1\n",
      "         569       0.00      0.00      0.00         0\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         4\n",
      "         572       0.00      0.00      0.00         2\n",
      "         573       0.00      0.00      0.00         2\n",
      "         574       0.00      0.00      0.00         2\n",
      "         575       0.00      0.00      0.00         5\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         1\n",
      "         578       0.00      0.00      0.00         0\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         2\n",
      "         582       0.00      0.00      0.00         1\n",
      "         583       0.00      0.00      0.00         2\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         0\n",
      "         586       0.00      0.00      0.00         1\n",
      "         587       0.00      0.00      0.00         1\n",
      "         588       0.00      0.00      0.00         0\n",
      "         589       0.00      0.00      0.00         0\n",
      "         590       0.00      0.00      0.00         0\n",
      "         650       0.98      0.86      0.92       145\n",
      "         660       0.00      0.00      0.00         1\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         0\n",
      "         667       0.00      0.00      0.00         0\n",
      "         668       0.00      0.00      0.00         1\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         0\n",
      "         671       0.00      0.00      0.00         1\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         2\n",
      "         674       0.00      0.00      0.00         0\n",
      "         675       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         0\n",
      "         678       0.00      0.00      0.00         0\n",
      "         679       0.00      0.00      0.00         0\n",
      "         680       0.00      0.00      0.00         1\n",
      "         681       0.00      0.00      0.00         1\n",
      "         682       0.00      0.00      0.00         0\n",
      "         683       0.00      0.00      0.00         1\n",
      "         684       0.00      0.00      0.00         2\n",
      "         685       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         1\n",
      "         687       0.00      0.00      0.00         2\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         1\n",
      "         690       0.00      0.00      0.00         0\n",
      "         750       0.86      0.95      0.90        99\n",
      "         760       0.00      0.00      0.00         0\n",
      "         761       0.00      0.00      0.00         1\n",
      "         762       0.00      0.00      0.00         3\n",
      "         763       0.00      0.00      0.00         0\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         1\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         1\n",
      "         771       0.00      0.00      0.00         1\n",
      "         772       0.00      0.00      0.00         0\n",
      "         773       0.00      0.00      0.00         1\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         0\n",
      "         777       0.00      0.00      0.00         1\n",
      "         778       0.00      0.00      0.00         2\n",
      "         779       0.00      0.00      0.00         1\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         1\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         1\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         1\n",
      "         788       0.00      0.00      0.00         0\n",
      "         789       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         0\n",
      "         850       1.00      0.69      0.82        74\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         0\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         0\n",
      "         865       0.00      0.00      0.00         1\n",
      "         866       0.00      0.00      0.00         0\n",
      "         868       0.00      0.00      0.00         1\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         1\n",
      "         875       0.00      0.00      0.00         0\n",
      "         876       0.00      0.00      0.00         0\n",
      "         878       0.00      0.00      0.00         1\n",
      "         879       0.00      0.00      0.00         0\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         0\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         0\n",
      "         888       0.00      0.00      0.00         1\n",
      "         889       0.00      0.00      0.00         0\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       1.00      0.58      0.74        36\n",
      "         960       0.00      0.00      0.00         0\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         1\n",
      "         967       0.00      0.00      0.00         1\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         0\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         1\n",
      "         978       0.00      0.00      0.00         0\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         0\n",
      "         986       0.00      0.00      0.00         1\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.84      0.90      1797\n",
      "   macro avg       0.05      0.04      0.04      1797\n",
      "weighted avg       0.87      0.84      0.85      1797\n",
      " samples avg       0.85      0.76      0.80      1797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ----------------------\n",
    "# Training with time tracking\n",
    "# ----------------------\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/50, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_SENN = end_time - start_time\n",
    "print(f\"\\n‚è±Ô∏è Training Time: {training_time_SENN:.2f} seconds\")\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation (5% Label Error Tolerance)\n",
    "# ----------------------\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # 5% error tolerance: allow some label mismatches\n",
    "        total_labels = labels.size(1)\n",
    "        mismatches = (preds != labels).sum(dim=1)  # per-sample mismatches\n",
    "        allowed_errors = int(0.01 * total_labels)\n",
    "\n",
    "        correct += (mismatches <= allowed_errors).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Micro F1 Score\n",
    "f1_SENN = f1_score(all_labels, all_preds, average='micro')\n",
    "accuracy_SENN = correct / total\n",
    "\n",
    "print(f\"\\n‚úÖ Micro F1 Score: {f1_SENN:.4f}\")\n",
    "print(f\"üéØ Match Accuracy with ‚â§1% Label Error: {accuracy_SENN:.4f}\")\n",
    "print(f\"‚è±Ô∏è Training Time: {training_time_SENN:.2f} seconds\")\n",
    "\n",
    "print(\"\\nüìã Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5b1c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:19:17.230967Z",
     "iopub.status.busy": "2025-05-26T15:19:17.230641Z",
     "iopub.status.idle": "2025-05-26T15:20:05.620016Z",
     "shell.execute_reply": "2025-05-26T15:20:05.618942Z"
    },
    "papermill": {
     "duration": 48.415958,
     "end_time": "2025-05-26T15:20:05.622740",
     "exception": false,
     "start_time": "2025-05-26T15:19:17.206782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1193\n",
      "Epoch 2/10, Loss: 0.0359\n",
      "Epoch 3/10, Loss: 0.0320\n",
      "Epoch 4/10, Loss: 0.0263\n",
      "Epoch 5/10, Loss: 0.0239\n",
      "Epoch 6/10, Loss: 0.0229\n",
      "Epoch 7/10, Loss: 0.0217\n",
      "Epoch 8/10, Loss: 0.0205\n",
      "Epoch 9/10, Loss: 0.0199\n",
      "Epoch 10/10, Loss: 0.0191\n",
      "\n",
      "‚úÖ Micro F1 Score: 0.8640\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         150       0.96      0.98      0.97       318\n",
      "         160       0.00      0.00      0.00         2\n",
      "         161       0.00      0.00      0.00         0\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       0.00      0.00      0.00         0\n",
      "         164       0.00      0.00      0.00         0\n",
      "         165       0.00      0.00      0.00         3\n",
      "         166       0.00      0.00      0.00         1\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       0.00      0.00      0.00         3\n",
      "         170       0.00      0.00      0.00         0\n",
      "         171       0.00      0.00      0.00         1\n",
      "         172       0.00      0.00      0.00         2\n",
      "         173       0.00      0.00      0.00         3\n",
      "         174       0.00      0.00      0.00         1\n",
      "         175       0.00      0.00      0.00         2\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         1\n",
      "         179       0.00      0.00      0.00         2\n",
      "         180       0.00      0.00      0.00         2\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         0\n",
      "         184       0.00      0.00      0.00         1\n",
      "         185       0.00      0.00      0.00         0\n",
      "         186       0.00      0.00      0.00         3\n",
      "         187       0.00      0.00      0.00         2\n",
      "         188       0.00      0.00      0.00         4\n",
      "         189       0.00      0.00      0.00         1\n",
      "         190       0.00      0.00      0.00         3\n",
      "         250       0.92      0.97      0.94       291\n",
      "         260       0.00      0.00      0.00         0\n",
      "         261       0.00      0.00      0.00         2\n",
      "         262       0.00      0.00      0.00         1\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         1\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         0\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         2\n",
      "         270       0.00      0.00      0.00         3\n",
      "         271       0.00      0.00      0.00         4\n",
      "         272       0.00      0.00      0.00         2\n",
      "         273       0.00      0.00      0.00         3\n",
      "         274       0.00      0.00      0.00         2\n",
      "         275       0.00      0.00      0.00         1\n",
      "         276       0.00      0.00      0.00         1\n",
      "         277       0.00      0.00      0.00         0\n",
      "         278       0.00      0.00      0.00         1\n",
      "         279       0.00      0.00      0.00         2\n",
      "         280       0.00      0.00      0.00         0\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         0\n",
      "         283       0.00      0.00      0.00         2\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         0\n",
      "         286       0.00      0.00      0.00         0\n",
      "         287       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         2\n",
      "         289       0.00      0.00      0.00         0\n",
      "         290       0.00      0.00      0.00         2\n",
      "         350       0.93      0.94      0.94       248\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         3\n",
      "         362       0.00      0.00      0.00         0\n",
      "         363       0.00      0.00      0.00         3\n",
      "         364       0.00      0.00      0.00         0\n",
      "         365       0.00      0.00      0.00         3\n",
      "         366       0.00      0.00      0.00         0\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.00      0.00      0.00         1\n",
      "         369       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         1\n",
      "         372       0.00      0.00      0.00         2\n",
      "         373       0.00      0.00      0.00         2\n",
      "         374       0.00      0.00      0.00         0\n",
      "         375       0.00      0.00      0.00         0\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         5\n",
      "         378       0.00      0.00      0.00         0\n",
      "         379       0.00      0.00      0.00         4\n",
      "         380       0.00      0.00      0.00         2\n",
      "         381       0.00      0.00      0.00         0\n",
      "         382       0.00      0.00      0.00         0\n",
      "         383       0.00      0.00      0.00         2\n",
      "         384       0.00      0.00      0.00         2\n",
      "         385       0.00      0.00      0.00         0\n",
      "         386       0.00      0.00      0.00         0\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       0.00      0.00      0.00         0\n",
      "         389       0.00      0.00      0.00         1\n",
      "         390       0.00      0.00      0.00         0\n",
      "         450       0.88      0.94      0.91       229\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       0.00      0.00      0.00         0\n",
      "         462       0.00      0.00      0.00         0\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         1\n",
      "         465       0.00      0.00      0.00         0\n",
      "         466       0.00      0.00      0.00         0\n",
      "         467       0.00      0.00      0.00         1\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         1\n",
      "         470       0.00      0.00      0.00         0\n",
      "         471       0.00      0.00      0.00         2\n",
      "         472       0.00      0.00      0.00         1\n",
      "         473       0.00      0.00      0.00         2\n",
      "         474       0.00      0.00      0.00         1\n",
      "         475       0.00      0.00      0.00         0\n",
      "         476       0.00      0.00      0.00         0\n",
      "         477       0.00      0.00      0.00         0\n",
      "         478       0.00      0.00      0.00         2\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         2\n",
      "         481       0.00      0.00      0.00         1\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         1\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         1\n",
      "         488       0.00      0.00      0.00         1\n",
      "         489       0.00      0.00      0.00         1\n",
      "         490       0.00      0.00      0.00         1\n",
      "         550       0.96      0.95      0.96       191\n",
      "         560       0.00      0.00      0.00         2\n",
      "         561       0.00      0.00      0.00         0\n",
      "         562       0.00      0.00      0.00         0\n",
      "         563       0.00      0.00      0.00         4\n",
      "         564       0.00      0.00      0.00         2\n",
      "         565       0.00      0.00      0.00         0\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       0.00      0.00      0.00         2\n",
      "         568       0.00      0.00      0.00         1\n",
      "         569       0.00      0.00      0.00         2\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         0\n",
      "         572       0.00      0.00      0.00         0\n",
      "         573       0.00      0.00      0.00         1\n",
      "         574       0.00      0.00      0.00         0\n",
      "         575       0.00      0.00      0.00         1\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         2\n",
      "         578       0.00      0.00      0.00         0\n",
      "         579       0.00      0.00      0.00         0\n",
      "         580       0.00      0.00      0.00         0\n",
      "         581       0.00      0.00      0.00         2\n",
      "         582       0.00      0.00      0.00         0\n",
      "         583       0.00      0.00      0.00         1\n",
      "         584       0.00      0.00      0.00         0\n",
      "         585       0.00      0.00      0.00         1\n",
      "         586       0.00      0.00      0.00         0\n",
      "         587       0.00      0.00      0.00         2\n",
      "         588       0.00      0.00      0.00         1\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         1\n",
      "         650       0.88      0.79      0.83       152\n",
      "         660       0.00      0.00      0.00         0\n",
      "         661       0.00      0.00      0.00         1\n",
      "         662       0.00      0.00      0.00         0\n",
      "         663       0.00      0.00      0.00         2\n",
      "         664       0.00      0.00      0.00         1\n",
      "         665       0.00      0.00      0.00         2\n",
      "         667       0.00      0.00      0.00         2\n",
      "         668       0.00      0.00      0.00         0\n",
      "         669       0.00      0.00      0.00         0\n",
      "         670       0.00      0.00      0.00         0\n",
      "         671       0.00      0.00      0.00         1\n",
      "         672       0.00      0.00      0.00         0\n",
      "         673       0.00      0.00      0.00         0\n",
      "         674       0.00      0.00      0.00         1\n",
      "         675       0.00      0.00      0.00         2\n",
      "         676       0.00      0.00      0.00         2\n",
      "         677       0.00      0.00      0.00         0\n",
      "         678       0.00      0.00      0.00         1\n",
      "         679       0.00      0.00      0.00         0\n",
      "         680       0.00      0.00      0.00         0\n",
      "         681       0.00      0.00      0.00         1\n",
      "         682       0.00      0.00      0.00         1\n",
      "         683       0.00      0.00      0.00         1\n",
      "         684       0.00      0.00      0.00         1\n",
      "         685       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         1\n",
      "         687       0.00      0.00      0.00         1\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         0\n",
      "         690       0.00      0.00      0.00         1\n",
      "         750       0.93      0.85      0.89       119\n",
      "         760       0.00      0.00      0.00         0\n",
      "         761       0.00      0.00      0.00         0\n",
      "         762       0.00      0.00      0.00         1\n",
      "         763       0.00      0.00      0.00         1\n",
      "         765       0.00      0.00      0.00         0\n",
      "         766       0.00      0.00      0.00         1\n",
      "         768       0.00      0.00      0.00         0\n",
      "         769       0.00      0.00      0.00         0\n",
      "         770       0.00      0.00      0.00         0\n",
      "         771       0.00      0.00      0.00         1\n",
      "         772       0.00      0.00      0.00         0\n",
      "         773       0.00      0.00      0.00         1\n",
      "         774       0.00      0.00      0.00         0\n",
      "         776       0.00      0.00      0.00         0\n",
      "         777       0.00      0.00      0.00         0\n",
      "         778       0.00      0.00      0.00         2\n",
      "         779       0.00      0.00      0.00         0\n",
      "         780       0.00      0.00      0.00         0\n",
      "         781       0.00      0.00      0.00         2\n",
      "         782       0.00      0.00      0.00         0\n",
      "         783       0.00      0.00      0.00         0\n",
      "         784       0.00      0.00      0.00         0\n",
      "         785       0.00      0.00      0.00         0\n",
      "         786       0.00      0.00      0.00         1\n",
      "         788       0.00      0.00      0.00         1\n",
      "         789       0.00      0.00      0.00         0\n",
      "         790       0.00      0.00      0.00         0\n",
      "         850       0.87      0.85      0.86        86\n",
      "         860       0.00      0.00      0.00         0\n",
      "         861       0.00      0.00      0.00         1\n",
      "         862       0.00      0.00      0.00         0\n",
      "         863       0.00      0.00      0.00         0\n",
      "         864       0.00      0.00      0.00         0\n",
      "         865       0.00      0.00      0.00         0\n",
      "         866       0.00      0.00      0.00         2\n",
      "         868       0.00      0.00      0.00         0\n",
      "         869       0.00      0.00      0.00         0\n",
      "         871       0.00      0.00      0.00         1\n",
      "         875       0.00      0.00      0.00         0\n",
      "         876       0.00      0.00      0.00         2\n",
      "         878       0.00      0.00      0.00         0\n",
      "         879       0.00      0.00      0.00         1\n",
      "         880       0.00      0.00      0.00         0\n",
      "         883       0.00      0.00      0.00         0\n",
      "         884       0.00      0.00      0.00         0\n",
      "         886       0.00      0.00      0.00         0\n",
      "         887       0.00      0.00      0.00         1\n",
      "         888       0.00      0.00      0.00         0\n",
      "         889       0.00      0.00      0.00         0\n",
      "         890       0.00      0.00      0.00         0\n",
      "         950       0.64      0.48      0.55        44\n",
      "         960       0.00      0.00      0.00         0\n",
      "         961       0.00      0.00      0.00         0\n",
      "         962       0.00      0.00      0.00         0\n",
      "         967       0.00      0.00      0.00         0\n",
      "         969       0.00      0.00      0.00         0\n",
      "         970       0.00      0.00      0.00         0\n",
      "         971       0.00      0.00      0.00         1\n",
      "         973       0.00      0.00      0.00         0\n",
      "         974       0.00      0.00      0.00         0\n",
      "         978       0.00      0.00      0.00         0\n",
      "         981       0.00      0.00      0.00         0\n",
      "         984       0.00      0.00      0.00         1\n",
      "         986       0.00      0.00      0.00         0\n",
      "         988       0.00      0.00      0.00         0\n",
      "         989       0.00      0.00      0.00         0\n",
      "         990       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.92      0.82      0.86      1889\n",
      "   macro avg       0.03      0.03      0.03      1889\n",
      "weighted avg       0.81      0.82      0.81      1889\n",
      " samples avg       0.84      0.74      0.77      1889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and Normalize Data\n",
    "# ------------------------------\n",
    "amplitude_df = pd.read_csv(\"/kaggle/input/secound/all_signals_1.csv\", header=None)\n",
    "frequency_df = pd.read_csv(\"/kaggle/input/secound/all_frequencies_1.csv\", header=None)\n",
    "\n",
    "X = amplitude_df.iloc[:, 1:].values.astype(np.float32)  # Skip timestamp column\n",
    "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Pad/Truncate and Reshape to (N, 1, 128, 128)\n",
    "# ------------------------------\n",
    "target_len = 128 * 128\n",
    "X_fixed = []\n",
    "\n",
    "for row in X:\n",
    "    if len(row) < target_len:\n",
    "        padded = np.pad(row, (0, target_len - len(row)), mode='constant')\n",
    "    else:\n",
    "        padded = row[:target_len]\n",
    "    X_fixed.append(padded)\n",
    "\n",
    "X_fixed = np.array(X_fixed).reshape(-1, 1, 128, 128).astype(np.float32)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Prepare Multi-Label Targets\n",
    "# ------------------------------\n",
    "y_raw = frequency_df.iloc[:, 1:].values  # Skip timestamp column\n",
    "label_sets = [list(set(row[row != 0])) for row in y_raw]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(label_sets)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Create Datasets and Loaders\n",
    "# ------------------------------\n",
    "X_tensor = torch.tensor(X_fixed)\n",
    "y_tensor = torch.tensor(y).float()\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Define CNN-based Multi-Label Classifier\n",
    "# ------------------------------\n",
    "class CNNFrequencyClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNFrequencyClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Sigmoid()  # For multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = CNNFrequencyClassifier(num_classes=y.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Training Loop\n",
    "# ------------------------------\n",
    "for epoch in range(10):  # You can increase if needed\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/10, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Evaluation\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).int()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "print(f\"\\n‚úÖ Micro F1 Score: {f1:.4f}\")\n",
    "print(\"\\nüìã Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=[str(l) for l in mlb.classes_]))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7446554,
     "sourceId": 11850954,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7516817,
     "sourceId": 11955659,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1357.954508,
   "end_time": "2025-05-26T15:20:08.894601",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-26T14:57:30.940093",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
